{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaX9bDq77cQM",
        "outputId": "6319e2bd-ab87-447b-ba86-a3670418233d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'/content/찐막.zip' 파일을 찾았습니다.\n",
            "'/content/uploadfolder' 폴더를 생성했습니다.\n",
            "'/content/찐막.zip' 압축을 '/content/uploadfolder'에 성공적으로 해제했습니다.\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 압축 파일 경로 및 해제할 폴더 경로 설정\n",
        "zip_file_path = '/content/찐막.zip'\n",
        "extract_dir = '/content/uploadfolder'  # 압축 해제할 폴더 경로\n",
        "\n",
        "# 압축 파일이 존재하는지 확인\n",
        "if os.path.exists(zip_file_path):\n",
        "    print(f\"'{zip_file_path}' 파일을 찾았습니다.\")\n",
        "\n",
        "    # 압축을 풀 폴더가 없으면 생성\n",
        "    if not os.path.exists(extract_dir):\n",
        "        os.makedirs(extract_dir)\n",
        "        print(f\"'{extract_dir}' 폴더를 생성했습니다.\")\n",
        "\n",
        "    # zipfile 라이브러리를 사용하여 압축 해제\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "        print(f\"'{zip_file_path}' 압축을 '{extract_dir}'에 성공적으로 해제했습니다.\")\n",
        "else:\n",
        "    print(f\"'{zip_file_path}' 파일을 찾을 수 없습니다. 경로를 확인하세요.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "\n",
        "# Custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(root_dir)\n",
        "        self.data = []\n",
        "\n",
        "        for label in range(len(self.classes)):\n",
        "            class_folder = os.path.join(root_dir, self.classes[label])\n",
        "            for filename in os.listdir(class_folder):\n",
        "                img_path = os.path.join(class_folder, filename)\n",
        "                self.data.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')  # 이미지 RGB로 변환\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# 경로 및 배치 크기 설정\n",
        "base_dir = './uploadfolder'  # 베이스 경로 (압축을 푼 폴더 경로)\n",
        "batch_size = 16\n",
        "\n",
        "# 통합된 train 및 valid 폴더 경로 설정\n",
        "train_dir = os.path.join(base_dir, '찐막', 'train')\n",
        "valid_dir = os.path.join(base_dir, '찐막', 'valid')\n",
        "\n",
        "# 데이터 증강 포함한 이미지 전처리\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(15),  # 더 큰 회전 각도\n",
        "    T.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 이미지 이동\n",
        "    T.GaussianBlur(kernel_size=3),  # Gaussian Blur 추가\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 학습 및 검증 데이터셋 생성\n",
        "train_dataset = CustomDataset(train_dir, transform=transform)\n",
        "valid_dataset = CustomDataset(valid_dir, transform=transform)\n",
        "\n",
        "# train 데이터셋이 비어있는지 확인\n",
        "if len(train_dataset) == 0:\n",
        "    raise ValueError(\"Train dataset is empty. Check if the images are correctly loaded.\")\n",
        "\n",
        "# 각 클래스의 데이터 개수 계산 (Counter 사용)\n",
        "class_counts = Counter([label for _, label in train_dataset])\n",
        "\n",
        "# 클래스별로 가중치를 부여 (데이터 개수의 역수 사용)\n",
        "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
        "\n",
        "# 각 샘플의 가중치를 리스트로 변환\n",
        "sample_weights = [class_weights[label] for _, label in train_dataset]\n",
        "\n",
        "# 샘플 가중치가 비어있는지 확인\n",
        "if len(sample_weights) == 0:\n",
        "    raise ValueError(\"Sample weights are empty. Check the dataset loading process.\")\n",
        "\n",
        "# WeightedRandomSampler 생성\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# WeightedRandomSampler를 적용한 DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 사전 학습된 resnext50_32x4d 모델 사용\n",
        "from torchvision.models import ResNeXt50_32X4D_Weights\n",
        "\n",
        "weights = ResNeXt50_32X4D_Weights.DEFAULT\n",
        "model = models.resnext50_32x4d(weights=weights)\n",
        "\n",
        "# 모든 레이어 학습 가능하도록 설정 (전이 학습 제외)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 출력 레이어를 분류하려는 클래스 수에 맞게 수정 (드롭아웃 비율 0.3으로 설정)\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.3),  # Dropout 비율을 0.3으로 조정\n",
        "    nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Early Stopping 설정\n",
        "best_accuracy = 0\n",
        "patience = 5  # 개선되지 않는 에포크를 허용하는 최대 수\n",
        "counter = 0\n",
        "\n",
        "# 손실 함수 및 AdamW 옵티마이저 설정 (학습률 0.0001로 감소)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "\n",
        "# 학습률 스케줄러 추가 (학습률 점진적 감소, gamma=0.5, step_size=10)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "# 학습 과정 (TQDM으로 진행 상황 시각화, 에포크 30)\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # TQDM으로 학습 진행 표시\n",
        "    train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for images, labels in train_loader_iter:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loader_iter.set_postfix(loss=running_loss / len(train_loader))\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Train Accuracy: {train_accuracy}%\")\n",
        "\n",
        "    # 학습률 스케줄러 업데이트\n",
        "    scheduler.step()\n",
        "\n",
        "    # 검증 평가\n",
        "    model.eval()\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    print(f'Validation Accuracy: {valid_accuracy}%')\n",
        "\n",
        "    # Early Stopping 기준\n",
        "    if valid_accuracy > best_accuracy:\n",
        "        best_accuracy = valid_accuracy\n",
        "        counter = 0  # 성능 개선 시 카운터 초기화\n",
        "        torch.save(model.state_dict(), 'best_model.pth')  # 최적의 모델 저장\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(\"Early stopping 적용\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtoppOjE7vgk",
        "outputId": "56bac2c2-e0f0-442c-a367-1d356cfe3ff8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnext50_32x4d-1a0047aa.pth\" to /root/.cache/torch/hub/checkpoints/resnext50_32x4d-1a0047aa.pth\n",
            "100%|██████████| 95.8M/95.8M [00:00<00:00, 163MB/s]\n",
            "Epoch 1/30: 100%|██████████| 25/25 [04:34<00:00, 10.98s/it, loss=0.628]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.6281898951530457, Train Accuracy: 65.80976863753213%\n",
            "Validation Accuracy: 61.72839506172839%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/30: 100%|██████████| 25/25 [04:15<00:00, 10.24s/it, loss=0.494]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/30, Loss: 0.4935920667648315, Train Accuracy: 81.49100257069409%\n",
            "Validation Accuracy: 67.90123456790124%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/30: 100%|██████████| 25/25 [04:14<00:00, 10.19s/it, loss=0.373]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/30, Loss: 0.372517032623291, Train Accuracy: 86.11825192802057%\n",
            "Validation Accuracy: 70.37037037037037%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/30: 100%|██████████| 25/25 [04:14<00:00, 10.17s/it, loss=0.351]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/30, Loss: 0.35121143341064454, Train Accuracy: 83.80462724935732%\n",
            "Validation Accuracy: 74.07407407407408%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/30: 100%|██████████| 25/25 [04:14<00:00, 10.20s/it, loss=0.237]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/30, Loss: 0.2366143596172333, Train Accuracy: 90.74550128534705%\n",
            "Validation Accuracy: 74.07407407407408%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/30: 100%|██████████| 25/25 [04:14<00:00, 10.18s/it, loss=0.252]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/30, Loss: 0.25178828299045564, Train Accuracy: 92.2879177377892%\n",
            "Validation Accuracy: 80.24691358024691%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/30: 100%|██████████| 25/25 [04:14<00:00, 10.18s/it, loss=0.155]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/30, Loss: 0.15467578411102295, Train Accuracy: 95.62982005141389%\n",
            "Validation Accuracy: 70.37037037037037%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/30: 100%|██████████| 25/25 [04:14<00:00, 10.19s/it, loss=0.175]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/30, Loss: 0.17509734980762004, Train Accuracy: 92.80205655526993%\n",
            "Validation Accuracy: 83.95061728395062%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/30: 100%|██████████| 25/25 [04:14<00:00, 10.18s/it, loss=0.147]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/30, Loss: 0.14662971943616868, Train Accuracy: 95.11568123393316%\n",
            "Validation Accuracy: 82.71604938271605%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/30: 100%|██████████| 25/25 [04:14<00:00, 10.20s/it, loss=0.132]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/30, Loss: 0.132050226777792, Train Accuracy: 96.1439588688946%\n",
            "Validation Accuracy: 72.8395061728395%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/30: 100%|██████████| 25/25 [04:15<00:00, 10.20s/it, loss=0.078]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/30, Loss: 0.07799242924898862, Train Accuracy: 97.4293059125964%\n",
            "Validation Accuracy: 80.24691358024691%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/30: 100%|██████████| 25/25 [04:14<00:00, 10.16s/it, loss=0.096]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/30, Loss: 0.09595256138592959, Train Accuracy: 96.91516709511568%\n",
            "Validation Accuracy: 81.48148148148148%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/30: 100%|██████████| 25/25 [04:14<00:00, 10.18s/it, loss=0.0813]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/30, Loss: 0.08129486303776502, Train Accuracy: 97.17223650385604%\n",
            "Validation Accuracy: 76.54320987654322%\n",
            "Early stopping 적용\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 모델이 예측한 결과와 실제 라벨을 기반으로 혼동 행렬 계산\n",
        "def calculate_confusion_matrix(model, dataloader, device):\n",
        "    model.eval()  # 평가 모드로 전환\n",
        "    true_labels = []\n",
        "    pred_labels = []\n",
        "\n",
        "    with torch.no_grad():  # 그래디언트 계산 비활성화 (추론 모드)\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            true_labels.extend(labels.cpu().numpy())  # 실제 라벨\n",
        "            pred_labels.extend(predicted.cpu().numpy())  # 예측된 라벨\n",
        "\n",
        "    # 혼동 행렬 계산\n",
        "    cm = confusion_matrix(true_labels, pred_labels)\n",
        "    return cm\n",
        "\n",
        "# 혼동 행렬 시각화\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "# 예시: 모델 평가 및 혼동 행렬 계산\n",
        "cm = calculate_confusion_matrix(model, valid_loader, device)\n",
        "\n",
        "# 클래스 이름 예시 (0, 1로 구분되는 경우)\n",
        "class_names = ['0', '1']\n",
        "\n",
        "# 혼동 행렬 시각화\n",
        "plot_confusion_matrix(cm, class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "czj4X69f8RNV",
        "outputId": "b3ee2be3-d63f-411d-fced-fb101e57f45b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F  # 이 부분을 추가\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "# Custom dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "        self.classes = os.listdir(root_dir)\n",
        "        self.data = []\n",
        "\n",
        "        for label in range(len(self.classes)):\n",
        "            class_folder = os.path.join(root_dir, self.classes[label])\n",
        "            for filename in os.listdir(class_folder):\n",
        "                img_path = os.path.join(class_folder, filename)\n",
        "                self.data.append((img_path, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.data[idx]\n",
        "        image = Image.open(img_path).convert('RGB')  # 이미지 RGB로 변환\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "# Focal Loss 함수 정의\n",
        "class FocalLoss(nn.Module):\n",
        "    def __init__(self, alpha=1, gamma=2, reduction='mean'):\n",
        "        super(FocalLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.gamma = gamma\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def forward(self, inputs, targets):\n",
        "        BCE_loss = F.cross_entropy(inputs, targets, reduction='none')  # torch.nn.functional에서 가져오는 cross_entropy\n",
        "        pt = torch.exp(-BCE_loss)  # 예측 확률\n",
        "        F_loss = self.alpha * (1 - pt) ** self.gamma * BCE_loss\n",
        "\n",
        "        if self.reduction == 'mean':\n",
        "            return F_loss.mean()\n",
        "        elif self.reduction == 'sum':\n",
        "            return F_loss.sum()\n",
        "        else:\n",
        "            return F_loss\n",
        "\n",
        "# 경로 및 배치 크기 설정\n",
        "base_dir = './uploadfolder'  # 베이스 경로 (압축을 푼 폴더 경로)\n",
        "batch_size = 16\n",
        "\n",
        "# 통합된 train 및 valid 폴더 경로 설정\n",
        "train_dir = os.path.join(base_dir, '찐막', 'train')\n",
        "valid_dir = os.path.join(base_dir, '찐막', 'valid')\n",
        "\n",
        "# 데이터 증강 포함한 이미지 전처리\n",
        "transform = T.Compose([\n",
        "    T.Resize((224, 224)),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomRotation(15),\n",
        "    T.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 이미지 이동\n",
        "    T.GaussianBlur(kernel_size=3),  # Gaussian Blur 추가\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 학습 및 검증 데이터셋 생성\n",
        "train_dataset = CustomDataset(train_dir, transform=transform)\n",
        "valid_dataset = CustomDataset(valid_dir, transform=transform)\n",
        "\n",
        "# train 데이터셋이 비어있는지 확인\n",
        "if len(train_dataset) == 0:\n",
        "    raise ValueError(\"Train dataset is empty. Check if the images are correctly loaded.\")\n",
        "\n",
        "# 각 클래스의 데이터 개수 계산 (Counter 사용)\n",
        "class_counts = Counter([label for _, label in train_dataset])\n",
        "\n",
        "# 클래스별로 가중치를 부여 (데이터 개수의 역수 사용)\n",
        "class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
        "sample_weights = [class_weights[label] for _, label in train_dataset]\n",
        "\n",
        "# WeightedRandomSampler 생성\n",
        "sampler = WeightedRandomSampler(weights=sample_weights, num_samples=len(sample_weights), replacement=True)\n",
        "\n",
        "# WeightedRandomSampler를 적용한 DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, sampler=sampler)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# 사전 학습된 resnext50_32x4d 모델 사용\n",
        "from torchvision.models import ResNeXt50_32X4D_Weights\n",
        "weights = ResNeXt50_32X4D_Weights.DEFAULT\n",
        "model = models.resnext50_32x4d(weights=weights)\n",
        "\n",
        "# 모든 레이어 학습 가능하도록 설정\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 출력 레이어를 분류하려는 클래스 수에 맞게 수정\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Dropout(0.3),\n",
        "    nn.Linear(model.fc.in_features, len(train_dataset.classes))\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Focal Loss 적용\n",
        "criterion = FocalLoss(alpha=1, gamma=2)\n",
        "\n",
        "# 손실 함수 및 AdamW 옵티마이저 설정 (학습률 0.0001로 감소)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
        "\n",
        "# 학습 과정\n",
        "num_epochs = 30\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    # TQDM으로 학습 진행 표시\n",
        "    train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "    for images, labels in train_loader_iter:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        train_loader_iter.set_postfix(loss=running_loss / len(train_loader))\n",
        "\n",
        "    train_accuracy = 100 * correct_train / total_train\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Train Accuracy: {train_accuracy}%\")\n",
        "\n",
        "    # 검증 평가\n",
        "    model.eval()\n",
        "    correct_valid = 0\n",
        "    total_valid = 0\n",
        "    with torch.no_grad():\n",
        "        for images, labels in valid_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_valid += labels.size(0)\n",
        "            correct_valid += (predicted == labels).sum().item()\n",
        "\n",
        "    valid_accuracy = 100 * correct_valid / total_valid\n",
        "    print(f'Validation Accuracy: {valid_accuracy}%')\n",
        "\n",
        "    # Early Stopping 기준\n",
        "    if valid_accuracy > best_accuracy:\n",
        "        best_accuracy = valid_accuracy\n",
        "        torch.save(model.state_dict(), 'best_model.pth')  # 최적의 모델 저장\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= 5:\n",
        "            print(\"Early stopping 적용\")\n",
        "            break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PBd5SQ6Ljvm",
        "outputId": "81ff8664-b29e-4642-d466-67b974956fc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/30: 100%|██████████| 25/25 [04:37<00:00, 11.08s/it, loss=0.169]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30, Loss: 0.16943849623203278, Train Accuracy: 62.98200514138817%\n",
            "Validation Accuracy: 64.19753086419753%\n",
            "Early stopping 적용\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z0z0zL_WQYj2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
