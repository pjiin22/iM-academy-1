{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 1. 앙상블 ① - 이미지 학습모델 3개"
      ],
      "metadata": {
        "id": "EJ1vqKKshGhU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "aUXVCYWqhF9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26d286fe-2ed4-447c-b71b-1333263b7af0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:10<00:00, 931635.91it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 132680.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1257750.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 2559522.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "# ResNet18\n",
        "class ResNetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNetModel, self).__init__()\n",
        "        self.model = resnet18(pretrained=False)\n",
        "        self.model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.model.fc = nn.Linear(self.model.fc.in_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "resnet_model = ResNetModel().to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nmgTwjzMc_i",
        "outputId": "29da0a73-6c68-47be-b953-11872a6b9139"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import densenet121\n",
        "\n",
        "# DenseNet121 모델\n",
        "class DenseNetModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DenseNetModel, self).__init__()\n",
        "        self.model = densenet121(pretrained=False)\n",
        "        self.model.features.conv0 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.model.classifier = nn.Linear(self.model.classifier.in_features, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "densenet_model = DenseNetModel().to(device)\n"
      ],
      "metadata": {
        "id": "3uHGVOHVMg7R"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.fc1 = nn.Linear(64 * 56 * 56, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.conv1(x))\n",
        "        x = nn.MaxPool2d(2, 2)(x)\n",
        "        x = nn.ReLU()(self.conv2(x))\n",
        "        x = nn.MaxPool2d(2, 2)(x)\n",
        "        x = x.view(-1, 64 * 56 * 56)\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "cnn_model = CNNModel().to(device)\n"
      ],
      "metadata": {
        "id": "PCleCQf_MjFZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "def train_model(model, optimizer, train_loader, epochs=1):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in tqdm(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "resnet_optimizer = optim.Adam(resnet_model.parameters(), lr=0.001)\n",
        "densenet_optimizer = optim.Adam(densenet_model.parameters(), lr=0.001)\n",
        "cnn_optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# 순차적으로 학습.\n",
        "# ResNet\n",
        "train_model(resnet_model, resnet_optimizer, train_loader)\n",
        "\n",
        "# DenseNet\n",
        "train_model(densenet_model, densenet_optimizer, train_loader)\n",
        "\n",
        "# CNN\n",
        "train_model(cnn_model, cnn_optimizer, train_loader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6r4nfWAoMkO5",
        "outputId": "c6bd8e44-30bb-423c-ad13-be77c99aff01"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [03:27<00:00,  4.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.026949115612626404\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [10:28<00:00,  1.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.05763620306964346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [02:00<00:00,  7.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.20287329705704107\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3개 모델 준비.\n",
        "models = [resnet_model, densenet_model, cnn_model]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in tqdm(test_loader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        outputs = [model(inputs) for model in models]\n",
        "        outputs = torch.mean(torch.stack(outputs), dim=0)\n",
        "\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "print(f'Ensemble Accuracy: {100 * correct / total:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8JF7ucYMpei",
        "outputId": "4b6c20a6-3a25-47e5-edbe-58d93cfc5cff"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:52<00:00,  3.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 99.48%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실습1) 와인데이터를 통한 MLP 모델 3개 클래스 분류 앙상블\n",
        "\n",
        "각 모델을 튜닝하여 성능을 높여보자."
      ],
      "metadata": {
        "id": "ZzFrgn-JO93u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "data = pd.read_csv('wine.csv')"
      ],
      "metadata": {
        "id": "WooRlcFRXpU0"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns=['class']).values\n",
        "y = data[['class']].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "wlFGcoH5O-Vv"
      },
      "execution_count": 173,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model1, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp_model1 = Model1()\n"
      ],
      "metadata": {
        "id": "os8Sy9ISPBaJ"
      },
      "execution_count": 174,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model2, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp_model2 = Model2()\n"
      ],
      "metadata": {
        "id": "cbVqA4CeO_Fw"
      },
      "execution_count": 175,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model3, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp_model3 = Model3()\n"
      ],
      "metadata": {
        "id": "2Cq9y6cvPAYQ"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "#criterion = nn.CrossEntropyLoss()\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "optimizer1 = optim.SGD(mlp_model1.parameters(), lr=0.01)\n",
        "optimizer2 = optim.SGD(mlp_model2.parameters(), lr=0.01)\n",
        "optimizer3 = optim.SGD(mlp_model3.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "C1yoveDPPESh"
      },
      "execution_count": 177,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer1.zero_grad()\n",
        "        outputs = mlp_model1(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"MLP Model 1, Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWF2uYjLPFCJ",
        "outputId": "0ddcdcb8-a203-41b9-c304-a0bd39d02bf7"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Model 1, Epoch 1, Loss: 0.6840338023697458\n",
            "MLP Model 1, Epoch 2, Loss: 0.626703308122914\n",
            "MLP Model 1, Epoch 3, Loss: 0.5803060553422789\n",
            "MLP Model 1, Epoch 4, Loss: 0.5371993042346908\n",
            "MLP Model 1, Epoch 5, Loss: 0.4905680530681843\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer2.zero_grad()\n",
        "        outputs = mlp_model2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"MLP Model 2, Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiJcnPaKPIIz",
        "outputId": "55323561-4848-4d19-ece0-8ed20c7a73e0"
      },
      "execution_count": 179,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Model 2, Epoch 1, Loss: 0.620295582748041\n",
            "MLP Model 2, Epoch 2, Loss: 0.5679339926417281\n",
            "MLP Model 2, Epoch 3, Loss: 0.5196893607697836\n",
            "MLP Model 2, Epoch 4, Loss: 0.46562711676446406\n",
            "MLP Model 2, Epoch 5, Loss: 0.3996948857859867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer3.zero_grad()\n",
        "        outputs = mlp_model3(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer3.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"MLP Model 3, Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G57l9xP1PI2h",
        "outputId": "a4cc9f5b-66db-4399-e5fe-45c6e398b3ab"
      },
      "execution_count": 180,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Model 3, Epoch 1, Loss: 0.6459484609161935\n",
            "MLP Model 3, Epoch 2, Loss: 0.5646071041502604\n",
            "MLP Model 3, Epoch 3, Loss: 0.4917079352024125\n",
            "MLP Model 3, Epoch 4, Loss: 0.41560937137138554\n",
            "MLP Model 3, Epoch 5, Loss: 0.33834616385582017\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "models = [mlp_model1, mlp_model2, mlp_model3]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = [model(inputs) for model in models]\n",
        "        outputs = torch.mean(torch.stack(outputs), dim=0)\n",
        "        outputs = torch.sigmoid(outputs)\n",
        "        predicted = (outputs >= 0.5).long().squeeze()\n",
        "        labels = labels.view_as(predicted)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Ensemble Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kYw8XZZZ9jZ",
        "outputId": "fee84001-570a-4688-efb4-7cf78f72271f"
      },
      "execution_count": 181,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 91.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 실습2) 와인데이터를 통한 MLP 모델 3개 퀄리티 분류 앙상블\n",
        "\n",
        "각 모델을 튜닝하여 성능을 높여보자."
      ],
      "metadata": {
        "id": "TqlkgaKRaq4y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "data = pd.read_csv('wine.csv')\n",
        "\n",
        "data['quality'] = LabelEncoder().fit_transform(data['quality'])"
      ],
      "metadata": {
        "id": "RvPiXEvmavly"
      },
      "execution_count": 183,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop(columns=['quality']).values\n",
        "y = data['quality'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = RobustScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "QNR-dp7Iaxjy"
      },
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model1, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp_model1 = Model1()\n"
      ],
      "metadata": {
        "id": "Wkyl69Tfaz1m"
      },
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model2, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp_model2 = Model2()\n"
      ],
      "metadata": {
        "id": "10tRtgDma0PT"
      },
      "execution_count": 200,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Model3, self).__init__()\n",
        "        self.fc1 = nn.Linear(12, 16)\n",
        "        self.fc2 = nn.Linear(16, 16)\n",
        "        self.fc3 = nn.Linear(16, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.fc1(x))\n",
        "        x = nn.ReLU()(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "mlp_model3 = Model3()\n"
      ],
      "metadata": {
        "id": "hAmS3opHa0kL"
      },
      "execution_count": 201,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "optimizer1 = optim.SGD(mlp_model1.parameters(), lr=0.01)\n",
        "optimizer2 = optim.SGD(mlp_model2.parameters(), lr=0.01)\n",
        "optimizer3 = optim.SGD(mlp_model3.parameters(), lr=0.01)\n"
      ],
      "metadata": {
        "id": "i8HugoUaa7Ka"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer1.zero_grad()\n",
        "        outputs = mlp_model1(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer1.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"MLP Model 1, Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOTTLG-Ta-CD",
        "outputId": "88eaa589-853d-4d16-b8c7-3b78cbe845ae"
      },
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Model 1, Epoch 1, Loss: 1.8964572912309228\n",
            "MLP Model 1, Epoch 2, Loss: 1.6932221607464115\n",
            "MLP Model 1, Epoch 3, Loss: 1.5409346586320458\n",
            "MLP Model 1, Epoch 4, Loss: 1.4328416309705594\n",
            "MLP Model 1, Epoch 5, Loss: 1.368042553343424\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer2.zero_grad()\n",
        "        outputs = mlp_model2(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"MLP Model 2, Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JIBIwTPa-WC",
        "outputId": "3c936326-ae62-487b-8df5-055a043991d1"
      },
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Model 2, Epoch 1, Loss: 1.8909091208039261\n",
            "MLP Model 2, Epoch 2, Loss: 1.6908259028341712\n",
            "MLP Model 2, Epoch 3, Loss: 1.5310511385522239\n",
            "MLP Model 2, Epoch 4, Loss: 1.427418538709966\n",
            "MLP Model 2, Epoch 5, Loss: 1.3688525063235586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        optimizer3.zero_grad()\n",
        "        outputs = mlp_model3(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer3.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f\"MLP Model 3, Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-aV5j4Ta-4x",
        "outputId": "afa58325-0da8-4695-bb78-63c17c6fe35f"
      },
      "execution_count": 205,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MLP Model 3, Epoch 1, Loss: 1.88933570501281\n",
            "MLP Model 3, Epoch 2, Loss: 1.681196342154247\n",
            "MLP Model 3, Epoch 3, Loss: 1.525530820939599\n",
            "MLP Model 3, Epoch 4, Loss: 1.42494510732046\n",
            "MLP Model 3, Epoch 5, Loss: 1.3616052618840846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models = [mlp_model1, mlp_model2, mlp_model3]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "        outputs = [model(inputs) for model in models]\n",
        "        outputs = torch.mean(torch.stack(outputs), dim=0)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "accuracy = 100 * correct / total\n",
        "print(f'Ensemble Accuracy: {accuracy:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z72XQX1LbJux",
        "outputId": "8fae6bcd-0172-4e21-b1bd-b2d3e61c3859"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Accuracy: 45.92%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. 멀티모달 학습\n",
        "\n",
        "1. MNIST 이미지와 벡터화된 MNIST 이미지를 CNN과 MLP로 특징 추출\n",
        "2. 그 후, 두가지 피처벡터를 합친다음에 FC로 최종 아웃풋 내는 모델에 전달"
      ],
      "metadata": {
        "id": "KOCB_K1Nd_Vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 10\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# CNN\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "# MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return x\n",
        "\n",
        "# 멀티모달모델\n",
        "class MultimodalModel(nn.Module):\n",
        "    def __init__(self, cnn_model, mlp_model, combined_hidden_size, num_classes):\n",
        "        super(MultimodalModel, self).__init__()\n",
        "        self.cnn = cnn_model\n",
        "        self.mlp = mlp_model\n",
        "        self.fc_combined = nn.Linear(128 + 128, combined_hidden_size)\n",
        "        self.fc_out = nn.Linear(combined_hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, image, flat_image):\n",
        "        image_features = self.cnn(image)\n",
        "        flat_image_features = self.mlp(flat_image)\n",
        "        combined = torch.cat((image_features, flat_image_features), dim=1)\n",
        "        x = F.relu(self.fc_combined(combined))\n",
        "        x = self.fc_out(x)\n",
        "        return x\n",
        "\n",
        "cnn_model = CNN()\n",
        "mlp_model = MLP(input_size=784, hidden_size=128)  # 28x28 이미지를 펼친 후 784차원 입력\n",
        "multimodal_model = MultimodalModel(cnn_model, mlp_model, combined_hidden_size=128, num_classes=10)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(multimodal_model.parameters(), lr=learning_rate)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "multimodal_model = multimodal_model.to(device)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    multimodal_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in tqdm(train_loader):\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # 이미지를 펼쳐서 정형 데이터로 변환\n",
        "        flat_images = images.view(images.size(0), -1).to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = multimodal_model(images, flat_images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "\n",
        "    multimodal_model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in tqdm(test_loader):\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # 이미지를 펼쳐서 정형 데이터로 변환\n",
        "            flat_images = images.view(images.size(0), -1).to(device)\n",
        "\n",
        "            outputs = multimodal_model(images, flat_images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    print(f'Validation Accuracy: {accuracy:.2f}%')\n"
      ],
      "metadata": {
        "id": "_OSrd_YV6WY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c89ae689-6f4c-4ac2-f900-a1ce89a8f98e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:48<00:00, 19.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.1936\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 29.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.26%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:52<00:00, 17.91it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Loss: 0.0533\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 31.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.43%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:48<00:00, 19.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Loss: 0.0381\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 34.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:50<00:00, 18.73it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Loss: 0.0302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 34.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.70%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:48<00:00, 19.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [5/10], Loss: 0.0243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 30.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.04%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:48<00:00, 19.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [6/10], Loss: 0.0182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 29.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.77%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:47<00:00, 19.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [7/10], Loss: 0.0169\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:06<00:00, 24.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.03%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:48<00:00, 19.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [8/10], Loss: 0.0139\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 34.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 98.74%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:48<00:00, 19.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Loss: 0.0126\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:05<00:00, 30.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 938/938 [00:48<00:00, 19.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/10], Loss: 0.0095\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 157/157 [00:04<00:00, 33.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 99.15%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}