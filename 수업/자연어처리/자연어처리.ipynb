{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "354ae2b2-1178-4eee-bbfa-f5195c8bf741",
   "metadata": {},
   "source": [
    "## 영어데이터\n",
    "nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dbbc0d7-418c-4da6-959f-7da5f807a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\dia\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\dia\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\dia\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\dia\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\dia\\anaconda3\\lib\\site-packages (from nltk) (4.66.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\dia\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "09d7a153-dab9-404b-94f2-60c6982190b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "#예시 문장\n",
    "TEXT = \"\"\"After reading the comments for this movie, I am not sure whether I should be angry, sad or sickened. Seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on CNN reports about Abu-Gharib makes me wonder about the state of intellectual stimulation in the world. At the time I type this the number of people in the US military: 1.4 million on Active Duty with another almost 900,000 in the Guard and Reserves for a total of roughly 2.3 million. The number of people indicted for abuses at at Abu-Gharib: Currently less than 20 That makes the total of people indicted .00083% of the total military. Even if you indict every single military member that ever stepped in to Abu-Gharib, you would not come close to making that a whole number.  The flaws in this movie would take YEARS to cover. I understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. In reality, the US military has been at its busiest when there are not conflicts going on. The military is the first called for disaster relief and humanitarian aid missions. When the tsunami hit Indonesia, devestating the region, the US military was the first on the scene. When the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. Within hours, food aid was reaching isolated villages. Within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. Hours and days, not weeks and months. Yes there are unscrupulous people in the US military. But then, there are in every walk of life, every occupation. But to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. One person on this website even went so far as to say that military members are in it for personal gain. Wow! Entry level personnel make just under $8.00 an hour assuming a 40 hour work week. Of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. That makes the pay well under minimum wage. So much for personal gain. I beg you, please make yourself familiar with the world around you. Go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. You would be surprised. The military no longer accepts people in lieu of prison time. They require a minimum of a GED and prefer a high school diploma. The middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9a1a9f3f-ac2d-4c70-bacc-5b1d9930ee1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_words = word_tokenize(TEXT) #단어토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527da1b-41b4-4b6e-8b2f-224392d61a10",
   "metadata": {},
   "source": [
    "## 단어 개수 세기\n",
    ": 빈도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "542a560b-3153-42dd-ac22-889ebb4362a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>WORD</th>\n",
       "      <th>FREQ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>.</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>,</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>of</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>and</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>disaster</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>relief</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>humanitarian</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>missions</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>advanced</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>278 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             WORD  FREQ\n",
       "2             the    30\n",
       "19              .    28\n",
       "7               ,    21\n",
       "22             of    15\n",
       "66            and    14\n",
       "..            ...   ...\n",
       "131      disaster     1\n",
       "132        relief     1\n",
       "133  humanitarian     1\n",
       "135      missions     1\n",
       "277      advanced     1\n",
       "\n",
       "[278 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dic = {}\n",
    "for i in  tokenized_words:\n",
    "    if i not in dic:\n",
    "        dic[i] = 1\n",
    "    else:\n",
    "        dic[i] += 1\n",
    "\n",
    "a = pd.DataFrame({\"WORD\" : dic.keys(), \"FREQ\" : dic.values()})\n",
    "a.sort_values(\"FREQ\", ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf6b7cba-b4cc-4e6b-9d58-b682d1db9dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'the': 30,\n",
       "         '.': 28,\n",
       "         ',': 21,\n",
       "         'of': 15,\n",
       "         'and': 14,\n",
       "         'to': 13,\n",
       "         'a': 12,\n",
       "         'military': 12,\n",
       "         'in': 12,\n",
       "         'people': 9,\n",
       "         'on': 9,\n",
       "         'are': 9,\n",
       "         'for': 7,\n",
       "         'this': 7,\n",
       "         'that': 6,\n",
       "         'I': 5,\n",
       "         'The': 5,\n",
       "         'you': 5,\n",
       "         'not': 4,\n",
       "         'or': 4,\n",
       "         'about': 4,\n",
       "         'US': 4,\n",
       "         'at': 4,\n",
       "         'every': 4,\n",
       "         'it': 4,\n",
       "         'make': 4,\n",
       "         'was': 4,\n",
       "         'movie': 3,\n",
       "         'be': 3,\n",
       "         'who': 3,\n",
       "         'they': 3,\n",
       "         'Abu-Gharib': 3,\n",
       "         'makes': 3,\n",
       "         'number': 3,\n",
       "         'million': 3,\n",
       "         'with': 3,\n",
       "         'total': 3,\n",
       "         'would': 3,\n",
       "         'an': 3,\n",
       "         'there': 3,\n",
       "         'days': 3,\n",
       "         'hour': 3,\n",
       "         'minimum': 3,\n",
       "         'get': 3,\n",
       "         'comments': 2,\n",
       "         ')': 2,\n",
       "         'know': 2,\n",
       "         'nothing': 2,\n",
       "         'base': 2,\n",
       "         'state': 2,\n",
       "         'world': 2,\n",
       "         'time': 2,\n",
       "         ':': 2,\n",
       "         '2.3': 2,\n",
       "         'indicted': 2,\n",
       "         'than': 2,\n",
       "         'That': 2,\n",
       "         \"'s\": 2,\n",
       "         'but': 2,\n",
       "         'reality': 2,\n",
       "         'is': 2,\n",
       "         'first': 2,\n",
       "         'aid': 2,\n",
       "         'When': 2,\n",
       "         'their': 2,\n",
       "         'Within': 2,\n",
       "         'hours': 2,\n",
       "         'food': 2,\n",
       "         'months': 2,\n",
       "         'But': 2,\n",
       "         'website': 2,\n",
       "         'men': 2,\n",
       "         'women': 2,\n",
       "         'so': 2,\n",
       "         'personal': 2,\n",
       "         'gain': 2,\n",
       "         'under': 2,\n",
       "         '40': 2,\n",
       "         'work': 2,\n",
       "         'week': 2,\n",
       "         'much': 2,\n",
       "         'ranks': 2,\n",
       "         'degrees': 2,\n",
       "         'After': 1,\n",
       "         'reading': 1,\n",
       "         'am': 1,\n",
       "         'sure': 1,\n",
       "         'whether': 1,\n",
       "         'should': 1,\n",
       "         'angry': 1,\n",
       "         'sad': 1,\n",
       "         'sickened': 1,\n",
       "         'Seeing': 1,\n",
       "         'typical': 1,\n",
       "         'absolutely': 1,\n",
       "         'b': 1,\n",
       "         'everything': 1,\n",
       "         'think': 1,\n",
       "         'movies': 1,\n",
       "         'like': 1,\n",
       "         'CNN': 1,\n",
       "         'reports': 1,\n",
       "         'me': 1,\n",
       "         'wonder': 1,\n",
       "         'intellectual': 1,\n",
       "         'stimulation': 1,\n",
       "         'At': 1,\n",
       "         'type': 1,\n",
       "         '1.4': 1,\n",
       "         'Active': 1,\n",
       "         'Duty': 1,\n",
       "         'another': 1,\n",
       "         'almost': 1,\n",
       "         '900,000': 1,\n",
       "         'Guard': 1,\n",
       "         'Reserves': 1,\n",
       "         'roughly': 1,\n",
       "         'abuses': 1,\n",
       "         'Currently': 1,\n",
       "         'less': 1,\n",
       "         '20': 1,\n",
       "         '.00083': 1,\n",
       "         '%': 1,\n",
       "         'Even': 1,\n",
       "         'if': 1,\n",
       "         'indict': 1,\n",
       "         'single': 1,\n",
       "         'member': 1,\n",
       "         'ever': 1,\n",
       "         'stepped': 1,\n",
       "         'come': 1,\n",
       "         'close': 1,\n",
       "         'making': 1,\n",
       "         'whole': 1,\n",
       "         'flaws': 1,\n",
       "         'take': 1,\n",
       "         'YEARS': 1,\n",
       "         'cover': 1,\n",
       "         'understand': 1,\n",
       "         'supposed': 1,\n",
       "         'sarcastic': 1,\n",
       "         'writer': 1,\n",
       "         'director': 1,\n",
       "         'trying': 1,\n",
       "         'commentary': 1,\n",
       "         'without': 1,\n",
       "         'enemy': 1,\n",
       "         'fight': 1,\n",
       "         'In': 1,\n",
       "         'has': 1,\n",
       "         'been': 1,\n",
       "         'its': 1,\n",
       "         'busiest': 1,\n",
       "         'when': 1,\n",
       "         'conflicts': 1,\n",
       "         'going': 1,\n",
       "         'called': 1,\n",
       "         'disaster': 1,\n",
       "         'relief': 1,\n",
       "         'humanitarian': 1,\n",
       "         'missions': 1,\n",
       "         'tsunami': 1,\n",
       "         'hit': 1,\n",
       "         'Indonesia': 1,\n",
       "         'devestating': 1,\n",
       "         'region': 1,\n",
       "         'scene': 1,\n",
       "         'chaos': 1,\n",
       "         'situation': 1,\n",
       "         'overwhelmed': 1,\n",
       "         'local': 1,\n",
       "         'governments': 1,\n",
       "         'leadership': 1,\n",
       "         'looked': 1,\n",
       "         'same': 1,\n",
       "         'mocks': 1,\n",
       "         'said': 1,\n",
       "         'happen': 1,\n",
       "         'reaching': 1,\n",
       "         'isolated': 1,\n",
       "         'villages': 1,\n",
       "         'airfields': 1,\n",
       "         'were': 1,\n",
       "         'built': 1,\n",
       "         'cargo': 1,\n",
       "         'aircraft': 1,\n",
       "         'started': 1,\n",
       "         'landing': 1,\n",
       "         'distribution': 1,\n",
       "         'system': 1,\n",
       "         'up': 1,\n",
       "         'running': 1,\n",
       "         'Hours': 1,\n",
       "         'weeks': 1,\n",
       "         'Yes': 1,\n",
       "         'unscrupulous': 1,\n",
       "         'then': 1,\n",
       "         'walk': 1,\n",
       "         'life': 1,\n",
       "         'occupation': 1,\n",
       "         'see': 1,\n",
       "         'decide': 1,\n",
       "         'all': 1,\n",
       "         'criminal': 1,\n",
       "         'minds': 1,\n",
       "         'thoughts': 1,\n",
       "         'destruction': 1,\n",
       "         'mayhem': 1,\n",
       "         'absolute': 1,\n",
       "         'disservice': 1,\n",
       "         'things': 1,\n",
       "         'do': 1,\n",
       "         'day': 1,\n",
       "         'One': 1,\n",
       "         'person': 1,\n",
       "         'even': 1,\n",
       "         'went': 1,\n",
       "         'far': 1,\n",
       "         'as': 1,\n",
       "         'say': 1,\n",
       "         'members': 1,\n",
       "         'Wow': 1,\n",
       "         '!': 1,\n",
       "         'Entry': 1,\n",
       "         'level': 1,\n",
       "         'personnel': 1,\n",
       "         'just': 1,\n",
       "         '$': 1,\n",
       "         '8.00': 1,\n",
       "         'assuming': 1,\n",
       "         'Of': 1,\n",
       "         'course': 1,\n",
       "         'many': 1,\n",
       "         'more': 1,\n",
       "         'those': 1,\n",
       "         'harm': 1,\n",
       "         'way': 1,\n",
       "         'typically': 1,\n",
       "         'put': 1,\n",
       "         '16-18': 1,\n",
       "         'end': 1,\n",
       "         'pay': 1,\n",
       "         'well': 1,\n",
       "         'wage': 1,\n",
       "         'So': 1,\n",
       "         'beg': 1,\n",
       "         'please': 1,\n",
       "         'yourself': 1,\n",
       "         'familiar': 1,\n",
       "         'around': 1,\n",
       "         'Go': 1,\n",
       "         'nearby': 1,\n",
       "         'visitor': 1,\n",
       "         'pass': 1,\n",
       "         'meet': 1,\n",
       "         'some': 1,\n",
       "         'quick': 1,\n",
       "         'disparage': 1,\n",
       "         'You': 1,\n",
       "         'surprised': 1,\n",
       "         'no': 1,\n",
       "         'longer': 1,\n",
       "         'accepts': 1,\n",
       "         'lieu': 1,\n",
       "         'prison': 1,\n",
       "         'They': 1,\n",
       "         'require': 1,\n",
       "         'GED': 1,\n",
       "         'prefer': 1,\n",
       "         'high': 1,\n",
       "         'school': 1,\n",
       "         'diploma': 1,\n",
       "         'middle': 1,\n",
       "         'expected': 1,\n",
       "         'undergraduate': 1,\n",
       "         'upper': 1,\n",
       "         'encouraged': 1,\n",
       "         'advanced': 1})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "vocab = Counter(tokenized_words)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f61f6178-69fd-4c7a-bada-7d426ff8b74a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['After', 'reading', 'the', 'comments', 'for', 'this', 'movie', ',', 'I', 'am', 'not', 'sure', 'whether', 'should', 'be', 'angry', 'sad', 'or', 'sickened', '.', 'Seeing', 'typical', 'of', 'people', 'who', 'a', ')', 'know', 'absolutely', 'nothing', 'about', 'military', 'b', 'base', 'everything', 'they', 'think', 'on', 'movies', 'like', 'CNN', 'reports', 'Abu-Gharib', 'makes', 'me', 'wonder', 'state', 'intellectual', 'stimulation', 'in', 'world', 'At', 'time', 'type', 'number', 'US', ':', '1.4', 'million', 'Active', 'Duty', 'with', 'another', 'almost', '900,000', 'Guard', 'and', 'Reserves', 'total', 'roughly', '2.3', 'The', 'indicted', 'abuses', 'at', 'Currently', 'less', 'than', '20', 'That', '.00083', '%', 'Even', 'if', 'you', 'indict', 'every', 'single', 'member', 'that', 'ever', 'stepped', 'to', 'would', 'come', 'close', 'making', 'whole', 'flaws', 'take', 'YEARS', 'cover', 'understand', 'it', \"'s\", 'supposed', 'sarcastic', 'but', 'reality', 'writer', 'director', 'are', 'trying', 'make', 'commentary', 'without', 'an', 'enemy', 'fight', 'In', 'has', 'been', 'its', 'busiest', 'when', 'there', 'conflicts', 'going', 'is', 'first', 'called', 'disaster', 'relief', 'humanitarian', 'aid', 'missions', 'When', 'tsunami', 'hit', 'Indonesia', 'devestating', 'region', 'was', 'scene', 'chaos', 'situation', 'overwhelmed', 'local', 'governments', 'leadership', 'looked', 'their', 'same', 'mocks', 'said', 'happen', 'Within', 'hours', 'food', 'reaching', 'isolated', 'villages', 'days', 'airfields', 'were', 'built', 'cargo', 'aircraft', 'started', 'landing', 'distribution', 'system', 'up', 'running', 'Hours', 'weeks', 'months', 'Yes', 'unscrupulous', 'But', 'then', 'walk', 'life', 'occupation', 'see', 'website', 'decide', 'men', 'women', 'all', 'criminal', 'minds', 'thoughts', 'destruction', 'mayhem', 'absolute', 'disservice', 'things', 'do', 'day', 'One', 'person', 'even', 'went', 'so', 'far', 'as', 'say', 'members', 'personal', 'gain', 'Wow', '!', 'Entry', 'level', 'personnel', 'just', 'under', '$', '8.00', 'hour', 'assuming', '40', 'work', 'week', 'Of', 'course', 'many', 'much', 'more', 'those', 'harm', 'way', 'typically', 'put', '16-18', 'end', 'pay', 'well', 'minimum', 'wage', 'So', 'beg', 'please', 'yourself', 'familiar', 'around', 'Go', 'nearby', 'get', 'visitor', 'pass', 'meet', 'some', 'quick', 'disparage', 'You', 'surprised', 'no', 'longer', 'accepts', 'lieu', 'prison', 'They', 'require', 'GED', 'prefer', 'high', 'school', 'diploma', 'middle', 'ranks', 'expected', 'undergraduate', 'degrees', 'upper', 'encouraged', 'advanced'])\n",
      "dict_values([1, 1, 30, 2, 7, 7, 3, 21, 5, 1, 4, 1, 1, 1, 3, 1, 1, 4, 1, 28, 1, 1, 15, 9, 3, 12, 2, 2, 1, 2, 4, 12, 1, 2, 1, 3, 1, 9, 1, 1, 1, 1, 3, 3, 1, 1, 2, 1, 1, 12, 2, 1, 2, 1, 3, 4, 2, 1, 3, 1, 1, 3, 1, 1, 1, 1, 14, 1, 3, 1, 2, 5, 2, 1, 4, 1, 1, 2, 1, 2, 1, 1, 1, 1, 5, 1, 4, 1, 1, 6, 1, 1, 13, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 1, 1, 2, 2, 1, 1, 9, 1, 4, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 2, 2, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 4, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 2, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 3, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "print(vocab.keys())\n",
    "\n",
    "print(vocab.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f00e8a-c046-4d14-b981-9ceb14753db5",
   "metadata": {},
   "source": [
    "##\n",
    "텍스트 마이닝 : 완성 후에 결과를 보고 고쳐야함\\\n",
    "토픽 모델링 : 빈도가 아주 많은게 문제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5a63f26f-985b-4b5a-95c7-ec5c88597c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1\n",
      "reading 1\n",
      "the 30\n",
      "comments 2\n",
      "for 7\n",
      "this 7\n",
      "movie 3\n",
      ", 21\n",
      "I 5\n",
      "am 1\n",
      "not 4\n",
      "sure 1\n",
      "whether 1\n",
      "should 1\n",
      "be 3\n",
      "angry 1\n",
      "sad 1\n",
      "or 4\n",
      "sickened 1\n",
      ". 28\n",
      "Seeing 1\n",
      "typical 1\n",
      "of 15\n",
      "people 9\n",
      "who 3\n",
      "a 12\n",
      ") 2\n",
      "know 2\n",
      "absolutely 1\n",
      "nothing 2\n",
      "about 4\n",
      "military 12\n",
      "b 1\n",
      "base 2\n",
      "everything 1\n",
      "they 3\n",
      "think 1\n",
      "on 9\n",
      "movies 1\n",
      "like 1\n",
      "CNN 1\n",
      "reports 1\n",
      "Abu-Gharib 3\n",
      "makes 3\n",
      "me 1\n",
      "wonder 1\n",
      "state 2\n",
      "intellectual 1\n",
      "stimulation 1\n",
      "in 12\n",
      "world 2\n",
      "At 1\n",
      "time 2\n",
      "type 1\n",
      "number 3\n",
      "US 4\n",
      ": 2\n",
      "1.4 1\n",
      "million 3\n",
      "Active 1\n",
      "Duty 1\n",
      "with 3\n",
      "another 1\n",
      "almost 1\n",
      "900,000 1\n",
      "Guard 1\n",
      "and 14\n",
      "Reserves 1\n",
      "total 3\n",
      "roughly 1\n",
      "2.3 2\n",
      "The 5\n",
      "indicted 2\n",
      "abuses 1\n",
      "at 4\n",
      "Currently 1\n",
      "less 1\n",
      "than 2\n",
      "20 1\n",
      "That 2\n",
      ".00083 1\n",
      "% 1\n",
      "Even 1\n",
      "if 1\n",
      "you 5\n",
      "indict 1\n",
      "every 4\n",
      "single 1\n",
      "member 1\n",
      "that 6\n",
      "ever 1\n",
      "stepped 1\n",
      "to 13\n",
      "would 3\n",
      "come 1\n",
      "close 1\n",
      "making 1\n",
      "whole 1\n",
      "flaws 1\n",
      "take 1\n",
      "YEARS 1\n",
      "cover 1\n",
      "understand 1\n",
      "it 4\n",
      "'s 2\n",
      "supposed 1\n",
      "sarcastic 1\n",
      "but 2\n",
      "reality 2\n",
      "writer 1\n",
      "director 1\n",
      "are 9\n",
      "trying 1\n",
      "make 4\n",
      "commentary 1\n",
      "without 1\n",
      "an 3\n",
      "enemy 1\n",
      "fight 1\n",
      "In 1\n",
      "has 1\n",
      "been 1\n",
      "its 1\n",
      "busiest 1\n",
      "when 1\n",
      "there 3\n",
      "conflicts 1\n",
      "going 1\n",
      "is 2\n",
      "first 2\n",
      "called 1\n",
      "disaster 1\n",
      "relief 1\n",
      "humanitarian 1\n",
      "aid 2\n",
      "missions 1\n",
      "When 2\n",
      "tsunami 1\n",
      "hit 1\n",
      "Indonesia 1\n",
      "devestating 1\n",
      "region 1\n",
      "was 4\n",
      "scene 1\n",
      "chaos 1\n",
      "situation 1\n",
      "overwhelmed 1\n",
      "local 1\n",
      "governments 1\n",
      "leadership 1\n",
      "looked 1\n",
      "their 2\n",
      "same 1\n",
      "mocks 1\n",
      "said 1\n",
      "happen 1\n",
      "Within 2\n",
      "hours 2\n",
      "food 2\n",
      "reaching 1\n",
      "isolated 1\n",
      "villages 1\n",
      "days 3\n",
      "airfields 1\n",
      "were 1\n",
      "built 1\n",
      "cargo 1\n",
      "aircraft 1\n",
      "started 1\n",
      "landing 1\n",
      "distribution 1\n",
      "system 1\n",
      "up 1\n",
      "running 1\n",
      "Hours 1\n",
      "weeks 1\n",
      "months 2\n",
      "Yes 1\n",
      "unscrupulous 1\n",
      "But 2\n",
      "then 1\n",
      "walk 1\n",
      "life 1\n",
      "occupation 1\n",
      "see 1\n",
      "website 2\n",
      "decide 1\n",
      "men 2\n",
      "women 2\n",
      "all 1\n",
      "criminal 1\n",
      "minds 1\n",
      "thoughts 1\n",
      "destruction 1\n",
      "mayhem 1\n",
      "absolute 1\n",
      "disservice 1\n",
      "things 1\n",
      "do 1\n",
      "day 1\n",
      "One 1\n",
      "person 1\n",
      "even 1\n",
      "went 1\n",
      "so 2\n",
      "far 1\n",
      "as 1\n",
      "say 1\n",
      "members 1\n",
      "personal 2\n",
      "gain 2\n",
      "Wow 1\n",
      "! 1\n",
      "Entry 1\n",
      "level 1\n",
      "personnel 1\n",
      "just 1\n",
      "under 2\n",
      "$ 1\n",
      "8.00 1\n",
      "hour 3\n",
      "assuming 1\n",
      "40 2\n",
      "work 2\n",
      "week 2\n",
      "Of 1\n",
      "course 1\n",
      "many 1\n",
      "much 2\n",
      "more 1\n",
      "those 1\n",
      "harm 1\n",
      "way 1\n",
      "typically 1\n",
      "put 1\n",
      "16-18 1\n",
      "end 1\n",
      "pay 1\n",
      "well 1\n",
      "minimum 3\n",
      "wage 1\n",
      "So 1\n",
      "beg 1\n",
      "please 1\n",
      "yourself 1\n",
      "familiar 1\n",
      "around 1\n",
      "Go 1\n",
      "nearby 1\n",
      "get 3\n",
      "visitor 1\n",
      "pass 1\n",
      "meet 1\n",
      "some 1\n",
      "quick 1\n",
      "disparage 1\n",
      "You 1\n",
      "surprised 1\n",
      "no 1\n",
      "longer 1\n",
      "accepts 1\n",
      "lieu 1\n",
      "prison 1\n",
      "They 1\n",
      "require 1\n",
      "GED 1\n",
      "prefer 1\n",
      "high 1\n",
      "school 1\n",
      "diploma 1\n",
      "middle 1\n",
      "ranks 2\n",
      "expected 1\n",
      "undergraduate 1\n",
      "degrees 2\n",
      "upper 1\n",
      "encouraged 1\n",
      "advanced 1\n"
     ]
    }
   ],
   "source": [
    "uncommon_words = []\n",
    "for key, value in vocab.items():\n",
    "    print(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb51ddb2-99f8-49e0-a979-16b659f799d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',', 'or', '.', 'of', 'people', 'who', 'a', 'about', 'the', 'military', 'or', 'who', 'they', 'they', 'on', 'this', 'or', 'on', 'about', 'Abu-Gharib', 'makes', 'about', 'the', 'of', 'in', 'the', '.', 'the', 'I', 'this', 'the', 'number', 'of', 'people', 'in', 'the', 'US', 'military', 'million', 'on', 'with', 'in', 'the', 'and', 'for', 'a', 'total', 'of', 'million', '.', 'The', 'number', 'of', 'people', 'for', 'at', 'at', 'Abu-Gharib', 'makes', 'the', 'total', 'of', 'people', 'of', 'the', 'total', 'military', '.', 'you', 'every', 'military', 'that', 'in', 'to', 'Abu-Gharib', ',', 'you', 'would', 'not', 'to', 'that', 'a', 'number', '.', 'The', 'in', 'this', 'movie', 'would', 'to', '.', 'I', 'that', 'it', 'to', 'be', ',', 'in', ',', 'the', 'and', 'are', 'to', 'make', 'about', 'the', 'of', 'the', 'military', 'an', 'to', '.', ',', 'the', 'US', 'military', 'at', 'there', 'are', 'not', 'on', '.', 'The', 'military', 'the', 'for', 'and', '.', 'the', ',', 'the', ',', 'the', 'US', 'military', 'was', 'the', 'on', 'the', '.', 'the', 'of', 'the', 'the', ',', 'it', 'was', 'military', 'who', 'at', 'people', ',', 'the', 'people', 'this', 'movie', ',', 'and', 'make', 'it', '.', ',', 'was', '.', 'days', ',', ',', 'and', 'a', 'was', 'and', '.', 'and', 'days', ',', 'not', 'and', '.', 'there', 'are', 'people', 'in', 'the', 'US', 'military', '.', ',', 'there', 'are', 'in', 'every', 'of', ',', 'every', '.', 'to', 'people', 'on', 'this', 'that', 'million', 'and', 'are', ',', 'with', 'on', 'of', 'or', 'an', 'to', 'the', 'that', 'they', 'every', '.', 'on', 'this', 'to', 'that', 'military', 'are', 'in', 'it', 'for', '.', 'make', 'an', 'hour', 'a', 'hour', '.', ',', 'a', 'and', 'in', 'in', 'hour', 'days', 'for', 'on', '.', 'makes', 'the', 'minimum', '.', 'for', '.', 'I', 'you', ',', 'make', 'with', 'the', 'you', '.', 'to', 'a', ',', 'get', 'a', 'and', 'of', 'the', 'and', 'you', 'are', 'to', '.', 'would', 'be', '.', 'The', 'military', 'people', 'in', 'of', '.', 'a', 'minimum', 'of', 'a', 'and', 'a', '.', 'The', 'are', 'to', 'get', 'a', 'minimum', 'of', 'and', 'the', 'are', 'to', 'get', '.']\n"
     ]
    }
   ],
   "source": [
    "#빈도가 2이하인 단어 제거\n",
    "cleaned_by_freq = []\n",
    "for word in tokenized_words:\n",
    "    if word not in uncommon_words and vocab[word] > 2:\n",
    "        cleaned_by_freq.append(word)\n",
    "\n",
    "print(cleaned_by_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e1a43c-4fdf-4670-a8cf-59ff57439cb9",
   "metadata": {},
   "source": [
    "## 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "950ed951-fefe-4bc3-8c70-5ae393175441",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'for', 'this', 'movie', 'not', 'people', 'who', 'about', 'the', 'military', 'who', 'they', 'they', 'this', 'about', 'Abu-Gharib', 'makes', 'about', 'the', 'the', 'the', 'this', 'the', 'number', 'people', 'the', 'military', 'million', 'with', 'the', 'and', 'for', 'total', 'million', 'The', 'number', 'people', 'for', 'Abu-Gharib', 'makes', 'the', 'total', 'people', 'the', 'total', 'military', 'you', 'every', 'military', 'that', 'Abu-Gharib', 'you', 'would', 'not', 'that', 'number', 'The', 'this', 'movie', 'would', 'that', 'the', 'and', 'are', 'make', 'about', 'the', 'the', 'military', 'the', 'military', 'there', 'are', 'not', 'The', 'military', 'the', 'for', 'and', 'the', 'the', 'the', 'military', 'was', 'the', 'the', 'the', 'the', 'the', 'was', 'military', 'who', 'people', 'the', 'people', 'this', 'movie', 'and', 'make', 'was', 'days', 'and', 'was', 'and', 'and', 'days', 'not', 'and', 'there', 'are', 'people', 'the', 'military', 'there', 'are', 'every', 'every', 'people', 'this', 'that', 'million', 'and', 'are', 'with', 'the', 'that', 'they', 'every', 'this', 'that', 'military', 'are', 'for', 'make', 'hour', 'hour', 'and', 'hour', 'days', 'for', 'makes', 'the', 'minimum', 'for', 'you', 'make', 'with', 'the', 'you', 'get', 'and', 'the', 'and', 'you', 'are', 'would', 'The', 'military', 'people', 'minimum', 'and', 'The', 'are', 'get', 'minimum', 'and', 'the', 'are', 'get']\n"
     ]
    }
   ],
   "source": [
    "#길이가 2이하인 단어 제거\n",
    "cleaned_by_freq_len = []\n",
    "for word in cleaned_by_freq:\n",
    "    if len(word) > 2:\n",
    "        cleaned_by_freq_len.append(word)\n",
    "\n",
    "print(cleaned_by_freq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7783e12-47ff-4e47-b239-6ec3b1999c63",
   "metadata": {},
   "source": [
    "## 단어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ef6e7a8b-00ce-449b-a3ad-3f2707bff335",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean = {',', 'I', 'be'}\n",
    "clened_by = [word for word in cleaned_by_freq if word not in clean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "acf292db-3498-4a7f-9e58-aab373e6a9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정제 전 :  ['the', 'for', 'this', 'movie', ',', 'I', 'not', 'I', 'be', ',']\n",
      "정제 후 : ['the', 'for', 'this', 'movie', 'not', 'or', '.', 'of', 'people', 'who']\n"
     ]
    }
   ],
   "source": [
    "# 콤마, I, be 같은것들 잘 제거 확인\n",
    "print(\"정제 전 : \", cleaned_by_freq[:10])\n",
    "print(\"정제 후 :\", clened_by[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "865784f1-03a3-42d1-b01a-e1a0e1b48c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'for',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'not',\n",
       " 'people',\n",
       " 'who',\n",
       " 'about',\n",
       " 'the',\n",
       " 'military',\n",
       " 'who',\n",
       " 'they',\n",
       " 'they',\n",
       " 'this',\n",
       " 'about',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'about',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'this',\n",
       " 'the',\n",
       " 'number',\n",
       " 'people',\n",
       " 'the',\n",
       " 'military',\n",
       " 'million',\n",
       " 'with',\n",
       " 'the',\n",
       " 'and',\n",
       " 'for',\n",
       " 'total',\n",
       " 'million',\n",
       " 'The',\n",
       " 'number',\n",
       " 'people',\n",
       " 'for',\n",
       " 'Abu-Gharib',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'total',\n",
       " 'people',\n",
       " 'the',\n",
       " 'total',\n",
       " 'military',\n",
       " 'you',\n",
       " 'every',\n",
       " 'military',\n",
       " 'that',\n",
       " 'Abu-Gharib',\n",
       " 'you',\n",
       " 'would',\n",
       " 'not',\n",
       " 'that',\n",
       " 'number',\n",
       " 'The',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'would',\n",
       " 'that',\n",
       " 'the',\n",
       " 'and',\n",
       " 'are',\n",
       " 'make',\n",
       " 'about',\n",
       " 'the',\n",
       " 'the',\n",
       " 'military',\n",
       " 'the',\n",
       " 'military',\n",
       " 'there',\n",
       " 'are',\n",
       " 'not',\n",
       " 'The',\n",
       " 'military',\n",
       " 'the',\n",
       " 'for',\n",
       " 'and',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'military',\n",
       " 'was',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'the',\n",
       " 'was',\n",
       " 'military',\n",
       " 'who',\n",
       " 'people',\n",
       " 'the',\n",
       " 'people',\n",
       " 'this',\n",
       " 'movie',\n",
       " 'and',\n",
       " 'make',\n",
       " 'was',\n",
       " 'days',\n",
       " 'and',\n",
       " 'was',\n",
       " 'and',\n",
       " 'and',\n",
       " 'days',\n",
       " 'not',\n",
       " 'and',\n",
       " 'there',\n",
       " 'are',\n",
       " 'people',\n",
       " 'the',\n",
       " 'military',\n",
       " 'there',\n",
       " 'are',\n",
       " 'every',\n",
       " 'every',\n",
       " 'people',\n",
       " 'this',\n",
       " 'that',\n",
       " 'million',\n",
       " 'and',\n",
       " 'are',\n",
       " 'with',\n",
       " 'the',\n",
       " 'that',\n",
       " 'they',\n",
       " 'every',\n",
       " 'this',\n",
       " 'that',\n",
       " 'military',\n",
       " 'are',\n",
       " 'for',\n",
       " 'make',\n",
       " 'hour',\n",
       " 'hour',\n",
       " 'and',\n",
       " 'hour',\n",
       " 'days',\n",
       " 'for',\n",
       " 'makes',\n",
       " 'the',\n",
       " 'minimum',\n",
       " 'for',\n",
       " 'you',\n",
       " 'make',\n",
       " 'with',\n",
       " 'the',\n",
       " 'you',\n",
       " 'get',\n",
       " 'and',\n",
       " 'the',\n",
       " 'and',\n",
       " 'you',\n",
       " 'are',\n",
       " 'would',\n",
       " 'The',\n",
       " 'military',\n",
       " 'people',\n",
       " 'minimum',\n",
       " 'and',\n",
       " 'The',\n",
       " 'are',\n",
       " 'get',\n",
       " 'minimum',\n",
       " 'and',\n",
       " 'the',\n",
       " 'are',\n",
       " 'get']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_by_freq(tokenized_words, cut_off_count):\n",
    "    vocab = Counter(tokenized_words)\n",
    "\n",
    "    uncommon_words = {key for key, value in vocab.items() if value <= cut_off_count}\n",
    "    cleaned_words = [word for word in tokenized_words if word not in uncommon_words]\n",
    "\n",
    "    return cleaned_words\n",
    "    \n",
    "def clean_by_len(tokenized_words, cut_off_length):\n",
    "    cleaned_words = []\n",
    "\n",
    "    for word in tokenized_words:\n",
    "        if len(word) > cut_off_length:\n",
    "            cleaned_words.append(word)\n",
    "\n",
    "    return cleaned_words\n",
    "\n",
    "cleaned_by_freq = clean_by_freq(tokenized_words, 2)\n",
    "cleaned_words = clean_by_len(cleaned_by_freq, 2)\n",
    "\n",
    "cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8964c0-e906-46c1-9d7c-449b75bdce7a",
   "metadata": {},
   "source": [
    "## 불용어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7cc9ddf1-5bde-456e-849e-d5c7d62c9dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'oh',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "added_stopwords = ['oh', 'the', 'i']\n",
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "stopwords_set.update(added_stopwords)\n",
    "stopwords_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "326b218a-4e9d-4178-be4f-cc7194935ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords_set.add(\"hello\") #추가\n",
    "stopwords_set.remove(\"the\") #지우기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "654f5708-8165-4058-861a-6a3527bc15ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "불용어 제거 전 : 169\n",
      "불용어 제거 후 : 97\n"
     ]
    }
   ],
   "source": [
    "#빈도가 적게 나온 단어, 길이가 짧은 단어, 불용어에 있는 단어 모두 제거\n",
    "cleaned_words = []\n",
    "\n",
    "for word in cleaned_by_freq_len:\n",
    "    if word not in stopwords_set:\n",
    "        cleaned_words.append(word)\n",
    "\n",
    "print(f\"불용어 제거 전 : {len(cleaned_by_freq_len)}\")\n",
    "print(f\"불용어 제거 후 : {len(cleaned_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fb21c394-0e02-43ee-a46e-58fe7601ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 함수\n",
    "def clean_by_stopwords(tokenized_words, stop_words_set):\n",
    "    cleaned_words = []\n",
    "    \n",
    "    for word in tokenized_words:\n",
    "        if word not in stop_words_set:\n",
    "            cleaned_words.append(word)\n",
    "            \n",
    "    return cleaned_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624472c-81bf-4eb6-8f94-db6b7f7b6302",
   "metadata": {},
   "source": [
    "## 정규화 \n",
    ": 소문자 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bd12b141-a4fb-4d6f-b604-2e8a66fb9ea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"after reading the comments for this movie, i am not sure whether i should be angry, sad or sickened. seeing comments typical of people who a)know absolutely nothing about the military or b)who base everything they think they know on movies like this or on cnn reports about abu-gharib makes me wonder about the state of intellectual stimulation in the world. at the time i type this the number of people in the us military: 1.4 million on active duty with another almost 900,000 in the guard and reserves for a total of roughly 2.3 million. the number of people indicted for abuses at at abu-gharib: currently less than 20 that makes the total of people indicted .00083% of the total military. even if you indict every single military member that ever stepped in to abu-gharib, you would not come close to making that a whole number.  the flaws in this movie would take years to cover. i understand that it's supposed to be sarcastic, but in reality, the writer and director are trying to make commentary about the state of the military without an enemy to fight. in reality, the us military has been at its busiest when there are not conflicts going on. the military is the first called for disaster relief and humanitarian aid missions. when the tsunami hit indonesia, devestating the region, the us military was the first on the scene. when the chaos of the situation overwhelmed the local governments, it was military leadership who looked at their people, the same people this movie mocks, and said make it happen. within hours, food aid was reaching isolated villages. within days, airfields were built, cargo aircraft started landing and a food distribution system was up and running. hours and days, not weeks and months. yes there are unscrupulous people in the us military. but then, there are in every walk of life, every occupation. but to see people on this website decide that 2.3 million men and women are all criminal, with nothing on their minds but thoughts of destruction or mayhem is an absolute disservice to the things that they do every day. one person on this website even went so far as to say that military members are in it for personal gain. wow! entry level personnel make just under $8.00 an hour assuming a 40 hour work week. of course, many work much more than 40 hours a week and those in harm's way typically put in 16-18 hour days for months on end. that makes the pay well under minimum wage. so much for personal gain. i beg you, please make yourself familiar with the world around you. go to a nearby base, get a visitor pass and meet some of the men and women you are so quick to disparage. you would be surprised. the military no longer accepts people in lieu of prison time. they require a minimum of a ged and prefer a high school diploma. the middle ranks are expected to get a minimum of undergraduate degrees and the upper ranks are encouraged to get advanced degrees.\""
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2dd0b-df32-41fd-96ac-c171aaa40212",
   "metadata": {},
   "source": [
    "## 규칙기반정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "86c8bee8-be8b-48fc-9ea1-fa0486063054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['she', 'became', 'a', 'USA', 'citizen', ',', ' Umm']\n"
     ]
    }
   ],
   "source": [
    "# US, U.S, USA & Um, Umm, Ummmmm로 다양하다면\n",
    "dic = {'US' : 'USA', \"U.S\" : \"USA\", \"Ummmm\" : \" Umm\"}\n",
    "text2 = \"she became a US citizen, Ummmm\"\n",
    "\n",
    "nomalized_words = []\n",
    "\n",
    "tokenized_words = word_tokenize(text2)\n",
    "\n",
    "for word in tokenized_words:\n",
    "    if word in dic.keys():\n",
    "        word = dic[word]\n",
    "    nomalized_words.append(word)\n",
    "\n",
    "print(nomalized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038010a5-e460-4d2c-b95c-64d2a5d4e331",
   "metadata": {},
   "source": [
    "## 어간 추출\n",
    "PorterStemmer : 단순하게 어미만 잘라줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "74338dd5-4b59-4021-93e8-c798239327b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['you', 'are', 'so', 'love', '.', 'i', 'am', 'love', 'you', 'now', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "text3 = \"you are so lovely. I am loving you now.\"\n",
    "tokenized_words = word_tokenize(text3)\n",
    "\n",
    "stemmed_words = []\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "for word in tokenized_words:\n",
    "    stem = porter.stem(word)\n",
    "    stemmed_words.append(stem)\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b750c3-0c64-4318-aacd-c73cce82a41c",
   "metadata": {},
   "source": [
    "## 실습1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "4de54051-f1de-4e34-9507-1256f536603c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"imdb.tsv\", sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "125f960a-eedb-4204-8f77-d74a893d0408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다 소문자 처리하기\n",
    "df['review'] = df['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "cf27fb40-0d4e-4afc-9812-0207b78444f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [watching, time, chasers, ,, it, obvious, that...\n",
       "1    [i, saw, this, film, about, 20, years, ago, an...\n",
       "2    [minor, spoilers, in, new, york, ,, joan, barn...\n",
       "3    [i, went, to, see, this, film, with, a, great,...\n",
       "4    [yes, ,, i, agree, with, everyone, on, this, s...\n",
       "5    [jennifer, ehle, was, sparkling, in, \\, '', pr...\n",
       "6    [amy, poehler, is, a, terrific, comedian, on, ...\n",
       "7    [a, plane, carrying, employees, of, a, large, ...\n",
       "8    [a, well, made, ,, gritty, science, fiction, m...\n",
       "9    [incredibly, dumb, and, utterly, predictable, ...\n",
       "Name: word_tokens, dtype: object"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['word_tokens'] = df['review'].str.lower()\n",
    "df['word_tokens'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4e41b88e-9085-4987-a981-04f8c81aae5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>word_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, really, bad, movie, like, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, went, jump, send, n't, jump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, movie, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, northam, wonderful, wonderful, ehle, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, author, book, author, autho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, ceo, search, rescue, mission, ceo, har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  watching time chasers, it obvious that it was ...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  yes, i agree with everyone on this site this m...   \n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6  amy poehler is a terrific comedian on saturday...   \n",
       "7  a plane carrying employees of a large biotech ...   \n",
       "8  a well made, gritty science fiction movie, it ...   \n",
       "9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                         word_tokens  \\\n",
       "0  [watching, time, chasers, ,, it, obvious, that...   \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...   \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...   \n",
       "3  [i, went, to, see, this, film, with, a, great,...   \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...   \n",
       "5  [jennifer, ehle, was, sparkling, in, \\, '', pr...   \n",
       "6  [amy, poehler, is, a, terrific, comedian, on, ...   \n",
       "7  [a, plane, carrying, employees, of, a, large, ...   \n",
       "8  [a, well, made, ,, gritty, science, fiction, m...   \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  [one, film, said, really, bad, movie, like, sa...  \n",
       "1                                       [film, film]  \n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...  \n",
       "3  [went, film, film, went, jump, send, n't, jump...  \n",
       "4  [site, movie, bad, even, movie, made, movie, s...  \n",
       "5  [ehle, northam, wonderful, wonderful, ehle, no...  \n",
       "6  [role, movie, n't, author, book, author, autho...  \n",
       "7  [plane, ceo, search, rescue, mission, ceo, har...  \n",
       "8  [gritty, movie, sci-fi, good, suspense, movie,...  \n",
       "9                                       [girl, girl]  "
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del df['Unnamed: 0']\n",
    "\n",
    "df['review'] = df['review'].str.lower()\n",
    "df[\"word_tokens\"] = df['review'].apply(word_tokenize)\n",
    "\n",
    "stopwords_set = set(stopwords.words(\"english\"))\n",
    "\n",
    "df[\"cleaned_tokens\"] = df['word_tokens'].apply(lambda x : clean_by_freq(x, 1))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x : clean_by_len(x, 2))\n",
    "df['cleaned_tokens'] = df['cleaned_tokens'].apply(lambda x : clean_by_stopwords(x, stopwords_set))\n",
    "\n",
    "#df['stemmed_tokens'] = df['cleaned_tokens'].apply(stemming_by_porter)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b06751-60af-4844-b50c-94d84466920d",
   "metadata": {},
   "source": [
    "## 실습2\n",
    "토큰화 전 문장단위 토큰화가 중요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "fec9f16d-44c7-40a1-81a8-1ab76a466124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"My email address is 'abcde@codeit.com'.\", 'Send it to Mr.Kim.']"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#문장단위 토큰화\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text4 = \"My email address is 'abcde@codeit.com'. Send it to Mr.Kim.\"\n",
    "sent_tokenize(text4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "aa61456e-7791-4314-b5df-c7594dbfa7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Can you forward my email to Mr.Kim?', 'Thank you!']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text5 = \"Can you forward my email to Mr.Kim? Thank you!\"\n",
    "sent_tokenize(text5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "41b7cf0f-5f2f-4796-adcf-5bdfde71b5b5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('After', 'IN'), ('reading', 'VBG'), ('the', 'DT'), ('comments', 'NNS'), ('for', 'IN'), ('this', 'DT'), ('movie', 'NN'), (',', ','), ('I', 'PRP'), ('am', 'VBP'), ('not', 'RB'), ('sure', 'JJ'), ('whether', 'IN'), ('I', 'PRP'), ('should', 'MD'), ('be', 'VB'), ('angry', 'JJ'), (',', ','), ('sad', 'JJ'), ('or', 'CC'), ('sickened', 'VBN'), ('.', '.'), ('Seeing', 'VBG'), ('comments', 'NNS'), ('typical', 'JJ'), ('of', 'IN'), ('people', 'NNS'), ('who', 'WP'), ('a', 'DT'), (')', ')'), ('know', 'VBP'), ('absolutely', 'RB'), ('nothing', 'NN'), ('about', 'IN'), ('the', 'DT'), ('military', 'NN'), ('or', 'CC'), ('b', 'NN'), (')', ')'), ('who', 'WP'), ('base', 'VBP'), ('everything', 'NN'), ('they', 'PRP'), ('think', 'VBP'), ('they', 'PRP'), ('know', 'VBP'), ('on', 'IN'), ('movies', 'NNS'), ('like', 'IN'), ('this', 'DT'), ('or', 'CC'), ('on', 'IN'), ('CNN', 'NNP'), ('reports', 'NNS'), ('about', 'IN'), ('Abu-Gharib', 'NNP'), ('makes', 'VBZ'), ('me', 'PRP'), ('wonder', 'VB'), ('about', 'IN'), ('the', 'DT'), ('state', 'NN'), ('of', 'IN'), ('intellectual', 'JJ'), ('stimulation', 'NN'), ('in', 'IN'), ('the', 'DT'), ('world', 'NN'), ('.', '.'), ('At', 'IN'), ('the', 'DT'), ('time', 'NN'), ('I', 'PRP'), ('type', 'VBP'), ('this', 'DT'), ('the', 'DT'), ('number', 'NN'), ('of', 'IN'), ('people', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('military', 'JJ'), (':', ':'), ('1.4', 'CD'), ('million', 'CD'), ('on', 'IN'), ('Active', 'NNP'), ('Duty', 'NNP'), ('with', 'IN'), ('another', 'DT'), ('almost', 'RB'), ('900,000', 'CD'), ('in', 'IN'), ('the', 'DT'), ('Guard', 'NNP'), ('and', 'CC'), ('Reserves', 'NNP'), ('for', 'IN'), ('a', 'DT'), ('total', 'NN'), ('of', 'IN'), ('roughly', 'RB'), ('2.3', 'CD'), ('million', 'CD'), ('.', '.'), ('The', 'DT'), ('number', 'NN'), ('of', 'IN'), ('people', 'NNS'), ('indicted', 'VBN'), ('for', 'IN'), ('abuses', 'NNS'), ('at', 'IN'), ('at', 'IN'), ('Abu-Gharib', 'NNP'), (':', ':'), ('Currently', 'RB'), ('less', 'JJR'), ('than', 'IN'), ('20', 'CD'), ('That', 'WDT'), ('makes', 'VBZ'), ('the', 'DT'), ('total', 'JJ'), ('of', 'IN'), ('people', 'NNS'), ('indicted', 'VBN'), ('.00083', 'CD'), ('%', 'NN'), ('of', 'IN'), ('the', 'DT'), ('total', 'JJ'), ('military', 'JJ'), ('.', '.'), ('Even', 'RB'), ('if', 'IN'), ('you', 'PRP'), ('indict', 'VBP'), ('every', 'DT'), ('single', 'JJ'), ('military', 'JJ'), ('member', 'NN'), ('that', 'IN'), ('ever', 'RB'), ('stepped', 'VBN'), ('in', 'IN'), ('to', 'TO'), ('Abu-Gharib', 'NNP'), (',', ','), ('you', 'PRP'), ('would', 'MD'), ('not', 'RB'), ('come', 'VB'), ('close', 'RB'), ('to', 'TO'), ('making', 'VBG'), ('that', 'IN'), ('a', 'DT'), ('whole', 'JJ'), ('number', 'NN'), ('.', '.'), ('The', 'DT'), ('flaws', 'NNS'), ('in', 'IN'), ('this', 'DT'), ('movie', 'NN'), ('would', 'MD'), ('take', 'VB'), ('YEARS', 'NNS'), ('to', 'TO'), ('cover', 'VB'), ('.', '.'), ('I', 'PRP'), ('understand', 'VBP'), ('that', 'IN'), ('it', 'PRP'), (\"'s\", 'VBZ'), ('supposed', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('sarcastic', 'JJ'), (',', ','), ('but', 'CC'), ('in', 'IN'), ('reality', 'NN'), (',', ','), ('the', 'DT'), ('writer', 'NN'), ('and', 'CC'), ('director', 'NN'), ('are', 'VBP'), ('trying', 'VBG'), ('to', 'TO'), ('make', 'VB'), ('commentary', 'JJ'), ('about', 'IN'), ('the', 'DT'), ('state', 'NN'), ('of', 'IN'), ('the', 'DT'), ('military', 'NN'), ('without', 'IN'), ('an', 'DT'), ('enemy', 'NN'), ('to', 'TO'), ('fight', 'VB'), ('.', '.'), ('In', 'IN'), ('reality', 'NN'), (',', ','), ('the', 'DT'), ('US', 'NNP'), ('military', 'NN'), ('has', 'VBZ'), ('been', 'VBN'), ('at', 'IN'), ('its', 'PRP$'), ('busiest', 'JJS'), ('when', 'WRB'), ('there', 'EX'), ('are', 'VBP'), ('not', 'RB'), ('conflicts', 'NNS'), ('going', 'VBG'), ('on', 'IN'), ('.', '.'), ('The', 'DT'), ('military', 'NN'), ('is', 'VBZ'), ('the', 'DT'), ('first', 'JJ'), ('called', 'VBN'), ('for', 'IN'), ('disaster', 'NN'), ('relief', 'NN'), ('and', 'CC'), ('humanitarian', 'JJ'), ('aid', 'NN'), ('missions', 'NNS'), ('.', '.'), ('When', 'WRB'), ('the', 'DT'), ('tsunami', 'NN'), ('hit', 'VBD'), ('Indonesia', 'NNP'), (',', ','), ('devestating', 'VBG'), ('the', 'DT'), ('region', 'NN'), (',', ','), ('the', 'DT'), ('US', 'NNP'), ('military', 'NN'), ('was', 'VBD'), ('the', 'DT'), ('first', 'JJ'), ('on', 'IN'), ('the', 'DT'), ('scene', 'NN'), ('.', '.'), ('When', 'WRB'), ('the', 'DT'), ('chaos', 'NN'), ('of', 'IN'), ('the', 'DT'), ('situation', 'NN'), ('overwhelmed', 'VBD'), ('the', 'DT'), ('local', 'JJ'), ('governments', 'NNS'), (',', ','), ('it', 'PRP'), ('was', 'VBD'), ('military', 'JJ'), ('leadership', 'NN'), ('who', 'WP'), ('looked', 'VBD'), ('at', 'IN'), ('their', 'PRP$'), ('people', 'NNS'), (',', ','), ('the', 'DT'), ('same', 'JJ'), ('people', 'NNS'), ('this', 'DT'), ('movie', 'NN'), ('mocks', 'VBZ'), (',', ','), ('and', 'CC'), ('said', 'VBD'), ('make', 'VB'), ('it', 'PRP'), ('happen', 'VB'), ('.', '.'), ('Within', 'IN'), ('hours', 'NNS'), (',', ','), ('food', 'NN'), ('aid', 'NN'), ('was', 'VBD'), ('reaching', 'VBG'), ('isolated', 'JJ'), ('villages', 'NNS'), ('.', '.'), ('Within', 'IN'), ('days', 'NNS'), (',', ','), ('airfields', 'NNS'), ('were', 'VBD'), ('built', 'VBN'), (',', ','), ('cargo', 'NN'), ('aircraft', 'NN'), ('started', 'VBD'), ('landing', 'VBG'), ('and', 'CC'), ('a', 'DT'), ('food', 'NN'), ('distribution', 'NN'), ('system', 'NN'), ('was', 'VBD'), ('up', 'RB'), ('and', 'CC'), ('running', 'VBG'), ('.', '.'), ('Hours', 'NNS'), ('and', 'CC'), ('days', 'NNS'), (',', ','), ('not', 'RB'), ('weeks', 'NNS'), ('and', 'CC'), ('months', 'NNS'), ('.', '.'), ('Yes', 'UH'), ('there', 'EX'), ('are', 'VBP'), ('unscrupulous', 'JJ'), ('people', 'NNS'), ('in', 'IN'), ('the', 'DT'), ('US', 'NNP'), ('military', 'JJ'), ('.', '.'), ('But', 'CC'), ('then', 'RB'), (',', ','), ('there', 'EX'), ('are', 'VBP'), ('in', 'IN'), ('every', 'DT'), ('walk', 'NN'), ('of', 'IN'), ('life', 'NN'), (',', ','), ('every', 'DT'), ('occupation', 'NN'), ('.', '.'), ('But', 'CC'), ('to', 'TO'), ('see', 'VB'), ('people', 'NNS'), ('on', 'IN'), ('this', 'DT'), ('website', 'JJ'), ('decide', 'NN'), ('that', 'IN'), ('2.3', 'CD'), ('million', 'CD'), ('men', 'NNS'), ('and', 'CC'), ('women', 'NNS'), ('are', 'VBP'), ('all', 'DT'), ('criminal', 'JJ'), (',', ','), ('with', 'IN'), ('nothing', 'NN'), ('on', 'IN'), ('their', 'PRP$'), ('minds', 'NNS'), ('but', 'CC'), ('thoughts', 'NNS'), ('of', 'IN'), ('destruction', 'NN'), ('or', 'CC'), ('mayhem', 'NN'), ('is', 'VBZ'), ('an', 'DT'), ('absolute', 'JJ'), ('disservice', 'NN'), ('to', 'TO'), ('the', 'DT'), ('things', 'NNS'), ('that', 'IN'), ('they', 'PRP'), ('do', 'VBP'), ('every', 'DT'), ('day', 'NN'), ('.', '.'), ('One', 'CD'), ('person', 'NN'), ('on', 'IN'), ('this', 'DT'), ('website', 'NN'), ('even', 'RB'), ('went', 'VBD'), ('so', 'RB'), ('far', 'RB'), ('as', 'IN'), ('to', 'TO'), ('say', 'VB'), ('that', 'IN'), ('military', 'JJ'), ('members', 'NNS'), ('are', 'VBP'), ('in', 'IN'), ('it', 'PRP'), ('for', 'IN'), ('personal', 'JJ'), ('gain', 'NN'), ('.', '.'), ('Wow', 'NN'), ('!', '.'), ('Entry', 'NNP'), ('level', 'NN'), ('personnel', 'NNS'), ('make', 'VBP'), ('just', 'RB'), ('under', 'IN'), ('$', '$'), ('8.00', 'CD'), ('an', 'DT'), ('hour', 'NN'), ('assuming', 'VBG'), ('a', 'DT'), ('40', 'CD'), ('hour', 'NN'), ('work', 'NN'), ('week', 'NN'), ('.', '.'), ('Of', 'IN'), ('course', 'NN'), (',', ','), ('many', 'JJ'), ('work', 'VBP'), ('much', 'RB'), ('more', 'JJR'), ('than', 'IN'), ('40', 'CD'), ('hours', 'NNS'), ('a', 'DT'), ('week', 'NN'), ('and', 'CC'), ('those', 'DT'), ('in', 'IN'), ('harm', 'NN'), (\"'s\", 'POS'), ('way', 'NN'), ('typically', 'RB'), ('put', 'VBD'), ('in', 'IN'), ('16-18', 'JJ'), ('hour', 'NN'), ('days', 'NNS'), ('for', 'IN'), ('months', 'NNS'), ('on', 'IN'), ('end', 'NN'), ('.', '.'), ('That', 'DT'), ('makes', 'VBZ'), ('the', 'DT'), ('pay', 'NN'), ('well', 'RB'), ('under', 'IN'), ('minimum', 'NN'), ('wage', 'NN'), ('.', '.'), ('So', 'RB'), ('much', 'JJ'), ('for', 'IN'), ('personal', 'JJ'), ('gain', 'NN'), ('.', '.'), ('I', 'PRP'), ('beg', 'VBP'), ('you', 'PRP'), (',', ','), ('please', 'VB'), ('make', 'VB'), ('yourself', 'PRP'), ('familiar', 'JJ'), ('with', 'IN'), ('the', 'DT'), ('world', 'NN'), ('around', 'IN'), ('you', 'PRP'), ('.', '.'), ('Go', 'VB'), ('to', 'TO'), ('a', 'DT'), ('nearby', 'JJ'), ('base', 'NN'), (',', ','), ('get', 'VB'), ('a', 'DT'), ('visitor', 'NN'), ('pass', 'NN'), ('and', 'CC'), ('meet', 'VB'), ('some', 'DT'), ('of', 'IN'), ('the', 'DT'), ('men', 'NNS'), ('and', 'CC'), ('women', 'NNS'), ('you', 'PRP'), ('are', 'VBP'), ('so', 'RB'), ('quick', 'JJ'), ('to', 'TO'), ('disparage', 'VB'), ('.', '.'), ('You', 'PRP'), ('would', 'MD'), ('be', 'VB'), ('surprised', 'VBN'), ('.', '.'), ('The', 'DT'), ('military', 'JJ'), ('no', 'DT'), ('longer', 'RBR'), ('accepts', 'VBZ'), ('people', 'NNS'), ('in', 'IN'), ('lieu', 'NN'), ('of', 'IN'), ('prison', 'NN'), ('time', 'NN'), ('.', '.'), ('They', 'PRP'), ('require', 'VBP'), ('a', 'DT'), ('minimum', 'NN'), ('of', 'IN'), ('a', 'DT'), ('GED', 'NNP'), ('and', 'CC'), ('prefer', 'VB'), ('a', 'DT'), ('high', 'JJ'), ('school', 'NN'), ('diploma', 'NN'), ('.', '.'), ('The', 'DT'), ('middle', 'JJ'), ('ranks', 'NNS'), ('are', 'VBP'), ('expected', 'VBN'), ('to', 'TO'), ('get', 'VB'), ('a', 'DT'), ('minimum', 'NN'), ('of', 'IN'), ('undergraduate', 'JJ'), ('degrees', 'NNS'), ('and', 'CC'), ('the', 'DT'), ('upper', 'JJ'), ('ranks', 'NNS'), ('are', 'VBP'), ('encouraged', 'VBN'), ('to', 'TO'), ('get', 'VB'), ('advanced', 'JJ'), ('degrees', 'NNS'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# 빈도 분석 / 명사, 동사, 형용사\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger') #뭐 다운 받아달라고 뜨면 다운받으면 됨\n",
    "\n",
    "pos_tagged_words = []\n",
    "\n",
    "tokenized_sents = sent_tokenize(TEXT)\n",
    "\n",
    "for sentence in tokenized_sents:\n",
    "    tokenized_words = word_tokenize(sentence)\n",
    "    pos_tags = pos_tag(tokenized_words)\n",
    "    pos_tagged_words += pos_tags\n",
    "\n",
    "print(pos_tagged_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "2cf4d61c-476b-4940-81e2-46b605c47e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 품사 태깅 함수\n",
    "def pos_tagger(tokenized_sents):\n",
    "    pos_tagged_words = []\n",
    "    for sentence in tokenized_sents:\n",
    "        # 단어 토큰화\n",
    "        tokenized_words = word_tokenize(sentence)\n",
    "        # 품사 태깅\n",
    "        pos_tagged = pos_tag(tokenized_words)\n",
    "        pos_tagged_words.extend(pos_tagged)\n",
    "    return pos_tagged_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4c140ff6-dd02-4472-a76b-d58d2f652827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hello', 'NN'), ('world', 'NN'), ('!', '.')]"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = \"hello world!\"\n",
    "tokenized_words = word_tokenize(t)\n",
    "tagged_words = pos_tagger(tokenized_words)\n",
    "tagged_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00637d69-e281-4cd7-9267-888938f769dc",
   "metadata": {},
   "source": [
    "## 원형(품사) 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "5d687c52-717b-4805-8da3-17cc06df7a2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "world\n",
      "!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DIA\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "for word, tag in tagged_words:\n",
    "    a = lemmatizer.lemmatize(word, wn.NOUN) #명사 정확히 찾기\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "74f0e4d0-cadb-4b67-ab34-9ea91be216c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#형용사, 명사, 부사, 동사만 추출하는 함수\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith(\"J\"):\n",
    "        return wn.ADJ\n",
    "    elif tag.startswith(\"N\"):\n",
    "        return wn.ADV\n",
    "    elif tag.startswith(\"R\"):\n",
    "        return wn.VERB\n",
    "    else:\n",
    "        return tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "5e148f48-c7a1-44af-808b-1aecdb2d4545",
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_lemmatizer(pos_tagged_words):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for word, tag in pos_tagged_words:\n",
    "        wn_tag = penn_to_wn(tag)\n",
    "        if wn_tag in (wn.NOUN, wn.ADJ, wn.ADV, wn.VERB):\n",
    "            stem = lemmatizer.lemmatize(word, wn_tag)\n",
    "            lemmatized_words.append(stem)\n",
    "        else:\n",
    "            lemmatized_words.append(word)\n",
    "    return lemmatized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "53b001f5-97a9-4dc7-85be-c93c2f318d86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "      <td>[(watching, VBG), (time, NN), (chasers, NNS), ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "      <td>[(jennifer, NN), (ehle, NN), (was, VBD), (spar...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "      <td>[(a, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  watching time chasers, it obvious that it was ...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  yes, i agree with everyone on this site this m...   \n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6  amy poehler is a terrific comedian on saturday...   \n",
       "7  a plane carrying employees of a large biotech ...   \n",
       "8  a well made, gritty science fiction movie, it ...   \n",
       "9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [watching time chasers, it obvious that it was...   \n",
       "1  [i saw this film about 20 years ago and rememb...   \n",
       "2  [minor spoilers in new york, joan barnard (elv...   \n",
       "3  [i went to see this film with a great deal of ...   \n",
       "4  [yes, i agree with everyone on this site this ...   \n",
       "5  [jennifer ehle was sparkling in \\\"pride and pr...   \n",
       "6  [amy poehler is a terrific comedian on saturda...   \n",
       "7  [a plane carrying employees of a large biotech...   \n",
       "8  [a well made, gritty science fiction movie, it...   \n",
       "9  [incredibly dumb and utterly predictable story...   \n",
       "\n",
       "                                   pos_tagged_tokens  \\\n",
       "0  [(watching, VBG), (time, NN), (chasers, NNS), ...   \n",
       "1  [(i, NN), (saw, VBD), (this, DT), (film, NN), ...   \n",
       "2  [(minor, JJ), (spoilers, NNS), (in, IN), (new,...   \n",
       "3  [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...   \n",
       "4  [(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...   \n",
       "5  [(jennifer, NN), (ehle, NN), (was, VBD), (spar...   \n",
       "6  [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...   \n",
       "7  [(a, DT), (plane, NN), (carrying, VBG), (emplo...   \n",
       "8  [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...   \n",
       "9  [(incredibly, RB), (dumb, JJ), (and, CC), (utt...   \n",
       "\n",
       "                                   lemmatized_tokens  \n",
       "0  [watching, time, chasers, ,, it, obvious, that...  \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...  \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...  \n",
       "3  [i, went, to, see, this, film, with, a, great,...  \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...  \n",
       "5  [jennifer, ehle, was, sparkling, in, \\, '', pr...  \n",
       "6  [amy, poehler, is, a, terrific, comedian, on, ...  \n",
       "7  [a, plane, carrying, employees, of, a, large, ...  \n",
       "8  [a, well, made, ,, gritty, science, fiction, m...  \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...  "
      ]
     },
     "execution_count": 257,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"imdb.tsv\", sep = '\\t')\n",
    "del df['Unnamed: 0']\n",
    "\n",
    "df['review'] = df['review'].str.lower()\n",
    "df['sent_tokens'] = df['review'].apply(sent_tokenize)\n",
    "df['pos_tagged_tokens'] = df['sent_tokens'].apply(pos_tagger)\n",
    "\n",
    "df['lemmatized_tokens'] = df['pos_tagged_tokens'].apply(words_lemmatizer)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "1ce538c9-4ca9-4d60-9373-ce06d7e46a08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sent_tokens</th>\n",
       "      <th>pos_tagged_tokens</th>\n",
       "      <th>lemmatized_tokens</th>\n",
       "      <th>cleaned_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>watching time chasers, it obvious that it was ...</td>\n",
       "      <td>[watching time chasers, it obvious that it was...</td>\n",
       "      <td>[(watching, VBG), (time, NN), (chasers, NNS), ...</td>\n",
       "      <td>[watching, time, chasers, ,, it, obvious, that...</td>\n",
       "      <td>[one, film, said, really, bad, movie, like, sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i saw this film about 20 years ago and remembe...</td>\n",
       "      <td>[i saw this film about 20 years ago and rememb...</td>\n",
       "      <td>[(i, NN), (saw, VBD), (this, DT), (film, NN), ...</td>\n",
       "      <td>[i, saw, this, film, about, 20, years, ago, an...</td>\n",
       "      <td>[film, film]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>minor spoilers in new york, joan barnard (elvi...</td>\n",
       "      <td>[minor spoilers in new york, joan barnard (elv...</td>\n",
       "      <td>[(minor, JJ), (spoilers, NNS), (in, IN), (new,...</td>\n",
       "      <td>[minor, spoilers, in, new, york, ,, joan, barn...</td>\n",
       "      <td>[new, york, joan, barnard, elvire, audrey, bar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i went to see this film with a great deal of e...</td>\n",
       "      <td>[i went to see this film with a great deal of ...</td>\n",
       "      <td>[(i, JJ), (went, VBD), (to, TO), (see, VB), (t...</td>\n",
       "      <td>[i, went, to, see, this, film, with, a, great,...</td>\n",
       "      <td>[went, film, film, went, jump, send, n't, jump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yes, i agree with everyone on this site this m...</td>\n",
       "      <td>[yes, i agree with everyone on this site this ...</td>\n",
       "      <td>[(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...</td>\n",
       "      <td>[yes, ,, i, agree, with, everyone, on, this, s...</td>\n",
       "      <td>[site, movie, bad, even, movie, made, movie, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>jennifer ehle was sparkling in \\\"pride and pre...</td>\n",
       "      <td>[jennifer ehle was sparkling in \\\"pride and pr...</td>\n",
       "      <td>[(jennifer, NN), (ehle, NN), (was, VBD), (spar...</td>\n",
       "      <td>[jennifer, ehle, was, sparkling, in, \\, '', pr...</td>\n",
       "      <td>[ehle, northam, wonderful, wonderful, ehle, no...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>amy poehler is a terrific comedian on saturday...</td>\n",
       "      <td>[amy poehler is a terrific comedian on saturda...</td>\n",
       "      <td>[(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...</td>\n",
       "      <td>[amy, poehler, is, a, terrific, comedian, on, ...</td>\n",
       "      <td>[role, movie, n't, author, book, funny, author...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>a plane carrying employees of a large biotech ...</td>\n",
       "      <td>[a plane carrying employees of a large biotech...</td>\n",
       "      <td>[(a, DT), (plane, NN), (carrying, VBG), (emplo...</td>\n",
       "      <td>[a, plane, carrying, employees, of, a, large, ...</td>\n",
       "      <td>[plane, ceo, search, rescue, mission, ceo, har...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>a well made, gritty science fiction movie, it ...</td>\n",
       "      <td>[a well made, gritty science fiction movie, it...</td>\n",
       "      <td>[(a, DT), (well, NN), (made, VBN), (,, ,), (gr...</td>\n",
       "      <td>[a, well, made, ,, gritty, science, fiction, m...</td>\n",
       "      <td>[gritty, movie, sci-fi, good, suspense, movie,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>incredibly dumb and utterly predictable story ...</td>\n",
       "      <td>[incredibly dumb and utterly predictable story...</td>\n",
       "      <td>[(incredibly, RB), (dumb, JJ), (and, CC), (utt...</td>\n",
       "      <td>[incredibly, dumb, and, utterly, predictable, ...</td>\n",
       "      <td>[girl, girl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  watching time chasers, it obvious that it was ...   \n",
       "1  i saw this film about 20 years ago and remembe...   \n",
       "2  minor spoilers in new york, joan barnard (elvi...   \n",
       "3  i went to see this film with a great deal of e...   \n",
       "4  yes, i agree with everyone on this site this m...   \n",
       "5  jennifer ehle was sparkling in \\\"pride and pre...   \n",
       "6  amy poehler is a terrific comedian on saturday...   \n",
       "7  a plane carrying employees of a large biotech ...   \n",
       "8  a well made, gritty science fiction movie, it ...   \n",
       "9  incredibly dumb and utterly predictable story ...   \n",
       "\n",
       "                                         sent_tokens  \\\n",
       "0  [watching time chasers, it obvious that it was...   \n",
       "1  [i saw this film about 20 years ago and rememb...   \n",
       "2  [minor spoilers in new york, joan barnard (elv...   \n",
       "3  [i went to see this film with a great deal of ...   \n",
       "4  [yes, i agree with everyone on this site this ...   \n",
       "5  [jennifer ehle was sparkling in \\\"pride and pr...   \n",
       "6  [amy poehler is a terrific comedian on saturda...   \n",
       "7  [a plane carrying employees of a large biotech...   \n",
       "8  [a well made, gritty science fiction movie, it...   \n",
       "9  [incredibly dumb and utterly predictable story...   \n",
       "\n",
       "                                   pos_tagged_tokens  \\\n",
       "0  [(watching, VBG), (time, NN), (chasers, NNS), ...   \n",
       "1  [(i, NN), (saw, VBD), (this, DT), (film, NN), ...   \n",
       "2  [(minor, JJ), (spoilers, NNS), (in, IN), (new,...   \n",
       "3  [(i, JJ), (went, VBD), (to, TO), (see, VB), (t...   \n",
       "4  [(yes, UH), (,, ,), (i, JJ), (agree, VBP), (wi...   \n",
       "5  [(jennifer, NN), (ehle, NN), (was, VBD), (spar...   \n",
       "6  [(amy, JJ), (poehler, NN), (is, VBZ), (a, DT),...   \n",
       "7  [(a, DT), (plane, NN), (carrying, VBG), (emplo...   \n",
       "8  [(a, DT), (well, NN), (made, VBN), (,, ,), (gr...   \n",
       "9  [(incredibly, RB), (dumb, JJ), (and, CC), (utt...   \n",
       "\n",
       "                                   lemmatized_tokens  \\\n",
       "0  [watching, time, chasers, ,, it, obvious, that...   \n",
       "1  [i, saw, this, film, about, 20, years, ago, an...   \n",
       "2  [minor, spoilers, in, new, york, ,, joan, barn...   \n",
       "3  [i, went, to, see, this, film, with, a, great,...   \n",
       "4  [yes, ,, i, agree, with, everyone, on, this, s...   \n",
       "5  [jennifer, ehle, was, sparkling, in, \\, '', pr...   \n",
       "6  [amy, poehler, is, a, terrific, comedian, on, ...   \n",
       "7  [a, plane, carrying, employees, of, a, large, ...   \n",
       "8  [a, well, made, ,, gritty, science, fiction, m...   \n",
       "9  [incredibly, dumb, and, utterly, predictable, ...   \n",
       "\n",
       "                                      cleaned_tokens  \n",
       "0  [one, film, said, really, bad, movie, like, sa...  \n",
       "1                                       [film, film]  \n",
       "2  [new, york, joan, barnard, elvire, audrey, bar...  \n",
       "3  [went, film, film, went, jump, send, n't, jump...  \n",
       "4  [site, movie, bad, even, movie, made, movie, s...  \n",
       "5  [ehle, northam, wonderful, wonderful, ehle, no...  \n",
       "6  [role, movie, n't, author, book, funny, author...  \n",
       "7  [plane, ceo, search, rescue, mission, ceo, har...  \n",
       "8  [gritty, movie, sci-fi, good, suspense, movie,...  \n",
       "9                                       [girl, girl]  "
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도 1이하 없애고 길이 2이하 없애고 불용어 처리까지 해서 나온 결과 확인하기\n",
    "df[\"lemmatized_tokens\"] = df['pos_tagged_tokens'].apply(words_lemmatizer)\n",
    "df[\"cleaned_tokens\"] = df[\"lemmatized_tokens\"].apply(lambda x : clean_by_freq(x, 1))\n",
    "df[\"cleaned_tokens\"] = df[\"cleaned_tokens\"].apply(lambda x : clean_by_len(x, 2))\n",
    "df[\"cleaned_tokens\"] = df[\"cleaned_tokens\"].apply(lambda x : clean_by_stopwords(x, stopwords_set))\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9842795a-8fcc-4f1f-bfc0-df0d48ebd00a",
   "metadata": {},
   "source": [
    "## 전처리 : 한문장으로 만들기\n",
    "sklearn : 2차원\\\n",
    "정답지 : 1차원\\\n",
    "토큰은 리스트 형태로 저장됨 -> 한문장으로 바꿔줘야함\\\n",
    "텍스트 마이닝(라이브러리)을 쓰려면 일단 문자형태이긴해야해서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "41f5fe67-030d-4421-bdb2-e5e65cbe7cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['combined_corpus'] = df['cleaned_tokens'].apply(lambda X : \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3dffcc16-d1c2-4833-a4be-baf4383a5ff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('movie', 16),\n",
       " ('film', 12),\n",
       " (\"n't\", 11),\n",
       " ('bad', 8),\n",
       " ('time', 8),\n",
       " ('reason', 8),\n",
       " ('jim', 7),\n",
       " ('good', 7),\n",
       " ('one', 6),\n",
       " ('like', 6)]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box = []\n",
    "for words in df['cleaned_tokens']:\n",
    "    box += words\n",
    "\n",
    "Counter(box).most_common(10) #Counter 첫글자 대문자로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804ce222-d5d4-4c37-93ba-c11d38adc479",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
